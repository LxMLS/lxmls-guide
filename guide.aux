\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {0}Basic Tutorials}{1}{chapter.0}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Python}{1}{section.0.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.1}Python Basics}{1}{subsection.0.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{Running Python code}{1}{section*.1}}
\citation{PER-GRA:2007}
\@writefile{toc}{\contentsline {subsubsection}{Help and Documentation}{2}{section*.2}}
\@writefile{toc}{\contentsline {subsubsection}{Exiting}{2}{section*.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.2}Python by Example}{2}{subsection.0.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{Data Structures}{2}{section*.4}}
\@writefile{toc}{\contentsline {subsubsection}{Loops and Indentation}{3}{section*.5}}
\@writefile{toc}{\contentsline {subsubsection}{Control Flow}{3}{section*.6}}
\@writefile{toc}{\contentsline {subsubsection}{Functions}{4}{section*.7}}
\@writefile{toc}{\contentsline {subsubsection}{Profiling}{4}{section*.8}}
\@writefile{toc}{\contentsline {subsubsection}{Debugging in Python}{5}{section*.9}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Basic pdb/ipdb commands, parentheses indicates abbreviation}}{5}{table.0.1}}
\newlabel{tb::pdbbasiccommands}{{1}{5}{Basic pdb/ipdb commands, parentheses indicates abbreviation}{table.0.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.3}Exceptions}{6}{subsection.0.1.3}}
\@writefile{toc}{\contentsline {subsubsection}{Extending basic Functionalities with Modules}{6}{section*.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.4}Matplotlib -- Plotting in Python}{7}{subsection.0.1.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.5}Numpy -- Scientific Computing with Python}{7}{subsection.0.1.5}}
\@writefile{toc}{\contentsline {subsubsection}{Multidimensional Arrays}{7}{section*.11}}
\@writefile{toc}{\contentsline {subsubsection}{Mathematical Operations}{8}{section*.12}}
\@writefile{toc}{\contentsline {section}{\numberline {0.2}Essential Linear Algebra}{9}{section.0.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.1}Notation}{9}{subsection.0.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.2}Some Matrix Operations and Properties}{10}{subsection.0.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.3}Norms}{11}{subsection.0.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.4}Linear Independence, Rank, and Orthogonal Matrices}{12}{subsection.0.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {0.3}Probability Theory}{12}{section.0.3}}
\newlabel{eq:condprob}{{2}{13}{Probability Theory}{equation.0.3.2}{}}
\newlabel{eq:totprob}{{3}{13}{Probability Theory}{equation.0.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.1}Probability distribution functions}{13}{subsection.0.3.1}}
\newlabel{bernoulli}{{0.3.2}{14}{Bernoulli}{subsection.0.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.2} Bernoulli}{14}{subsection.0.3.2}}
\newlabel{bernoulli-eq}{{0.3.2}{14}{Bernoulli}{subsection.0.3.2}{}}
\newlabel{binomial}{{0.3.3}{14}{Binomial}{subsection.0.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.3} Binomial}{14}{subsection.0.3.3}}
\newlabel{categorical}{{0.3.4}{14}{Categorical}{subsection.0.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.4} Categorical}{14}{subsection.0.3.4}}
\newlabel{multinomial}{{0.3.5}{14}{Multinomial}{subsection.0.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.5} Multinomial}{14}{subsection.0.3.5}}
\newlabel{gaussian}{{0.3.6}{15}{Gaussian Distribution}{subsection.0.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.6} Gaussian Distribution}{15}{subsection.0.3.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Normal density for two sets of parameter values.}}{15}{figure.0.1}}
\newlabel{fig:normaldist}{{1}{15}{Normal density for two sets of parameter values}{figure.0.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.7}Maximum Likelihood Estimation}{15}{subsection.0.3.7}}
\citation{Nocedal1999}
\citation{bertsekas1995np}
\citation{boyd2004convex}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.8}Conjugate Priors}{17}{subsection.0.3.8}}
\newlabel{numerical_optimization}{{0.4}{17}{Numerical optimization}{section.0.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {0.4}Numerical optimization}{17}{section.0.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.4.1}Convex Functions}{17}{subsection.0.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.4.2}Derivative and Gradient}{17}{subsection.0.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Illustration of a convex function. The line segment between any two points on the graph lies entirely above the curve.}}{18}{figure.0.2}}
\newlabel{fig:convexfn}{{2}{18}{Illustration of a convex function. The line segment between any two points on the graph lies entirely above the curve}{figure.0.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Illustration of a non-convex function. Note the line segment intersecting the curve. }}{18}{figure.0.3}}
\newlabel{fig:nonconvexfn}{{3}{18}{Illustration of a non-convex function. Note the line segment intersecting the curve}{figure.0.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Some derivative examples}}{18}{table.0.2}}
\newlabel{tb::derivatives}{{2}{18}{Some derivative examples}{table.0.2}{}}
\citation{Nocedal1999}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Illustration of the gradient of the function $f(x^2)$ at three different points $x = [-2,0.2]$. Note that at point $x = 0$ the gradient is zero which corresponds to the minimum of the function.}}{19}{figure.0.4}}
\newlabel{fig:tangents}{{4}{19}{Illustration of the gradient of the function $f(x^2)$ at three different points $x = [-2,0.2]$. Note that at point $x = 0$ the gradient is zero which corresponds to the minimum of the function}{figure.0.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.4.3}Gradient Based Methods}{19}{subsection.0.4.3}}
\citation{Nocedal1999}
\newlabel{alg:graddescent}{{1}{20}{Gradient Based Methods}{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gradient Descent}}{20}{algorithm.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Illustration of gradient descent. The blue circles correspond to contours of the function (each blue circle is a set of points which have the same function value), while the red lines correspond to steps taken in the negative gradient direction.}}{21}{figure.0.5}}
\newlabel{fig:graddescent}{{5}{21}{Illustration of gradient descent. The blue circles correspond to contours of the function (each blue circle is a set of points which have the same function value), while the red lines correspond to steps taken in the negative gradient direction}{figure.0.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Example of running gradient descent starting on point $x_0 = -8$ for function $f(x) = (x+2)^2 - 16 \qopname  \relax o{exp}\left ( -(x-2)^2 \right )$. The function is represented in blue, while the points of the minimizing sequence are displayed as green plus signs.}}{22}{figure.0.6}}
\newlabel{fig:gradex1}{{6}{22}{Example of running gradient descent starting on point $x_0 = -8$ for function $f(x) = (x+2)^2 - 16 \exp \left ( -(x-2)^2 \right )$. The function is represented in blue, while the points of the minimizing sequence are displayed as green plus signs}{figure.0.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {0.5}Python Exercises}{22}{section.0.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.5.1}Numpy and Matplotlib}{22}{subsection.0.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Example of running gradient descent starting on point $x_0 = 8$ for function $f(x) = (x+2)^2 - 16 \qopname  \relax o{exp}\left ( -(x-2)^2 \right )$. The function is represented in blue, while the points of the minimizing sequence are displayed as green plus signs.}}{23}{figure.0.7}}
\newlabel{fig:gradex2}{{7}{23}{Example of running gradient descent starting on point $x_0 = 8$ for function $f(x) = (x+2)^2 - 16 \exp \left ( -(x-2)^2 \right )$. The function is represented in blue, while the points of the minimizing sequence are displayed as green plus signs}{figure.0.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.5.2}Debugging}{24}{subsection.0.5.2}}
\citation{Mitchell1997}
\citation{Duda2001}
\citation{Schoelkopf2002}
\citation{Joachims2002}
\citation{Bishop2006}
\citation{Manning2008}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Classification}{26}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{day:classification}{{1}{26}{Classification}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Notation}{26}{section.1.1}}
\newlabel{s::naiveBayes}{{1.2}{27}{Generative Classifiers: Na\"{i}ve Bayes}{section.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Generative Classifiers: Na\"{i}ve Bayes}{27}{section.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces  Example of a dataset together with the corresponding Bayes optimal decision boundary. The input set consists in points in the real plane, $\EuScript  {X} = \EuScript  {R^2}$, and the output set consists of two classes (Red and Blue). Training points are represented as squares, while test points are represented as circles.}}{27}{figure.1.1}}
\newlabel{simpleDataSet_bo}{{1.1}{27}{Example of a dataset together with the corresponding Bayes optimal decision boundary. The input set consists in points in the real plane, $\mathcal {X} = \mathcal {R^2}$, and the output set consists of two classes (Red and Blue). Training points are represented as squares, while test points are represented as circles}{figure.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Training and Inference}{28}{subsection.1.2.1}}
\newlabel{eq:argmax}{{1.4}{28}{Training and Inference}{equation.1.2.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Example: Multinomial Na\"{i}ve Bayes for Document Classification}{28}{subsection.1.2.2}}
\citation{blitzer2007biographies}
\citation{Manning1999}
\citation{Manning2008}
\newlabel{eq:mlemultinomial}{{1.9}{29}{Example: Multinomial Na\"{i}ve Bayes for Document Classification}{equation.1.2.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Assignment}{29}{section.1.3}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Discriminative Classifiers}{30}{section.1.4}}
\newlabel{sec:linearclass}{{1.4}{30}{Discriminative Classifiers}{section.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Features}{30}{subsection.1.4.1}}
\newlabel{eq:jointfeatsimple}{{1.16}{30}{Features}{equation.1.4.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Inference}{30}{subsection.1.4.2}}
\citation{Rosenblatt1958}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Online Discriminative Algorithms}{31}{subsection.1.4.3}}
\newlabel{s:perceptron}{{1.4.3}{31}{Perceptron}{section*.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{ Perceptron}{31}{section*.14}}
\@writefile{tdo}{\contentsline {todo}{\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \parsep 4\p@ plus2\p@ minus\p@ \topsep 8\p@ plus2\p@ minus4\p@ \itemsep 4\p@ plus2\p@ minus\p@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip O preceptron algorithm consiste no metodo de gradiente quando a funÁ„o de custo ¥e o erro quadratico, certo? Assim, pode ter varios passos (neste caso fixou-se o valor do passo em 1), e tambÈm tem uma vers„o bach.}{31}{section*.15}}
\pgfsyspdfmark {pgfid1}{29396239}{11003760}
\pgfsyspdfmark {pgfid4}{37625178}{10999994}
\pgfsyspdfmark {pgfid5}{39100543}{10773240}
\citation{Crammer2002}
\citation{Crammer2006}
\newlabel{alg:perceptron}{{2}{32}{Perceptron}{algorithm.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces  Averaged perceptron}}{32}{algorithm.2}}
\@writefile{tdo}{\contentsline {todo}{Falta definir o sd ("`simple dataset"') anterirormente. Apenas se definiu o dataset da Amazon.}{32}{section*.16}}
\pgfsyspdfmark {pgfid6}{5367761}{34073974}
\pgfsyspdfmark {pgfid9}{37625178}{34063818}
\pgfsyspdfmark {pgfid10}{39100543}{33837064}
\@writefile{toc}{\contentsline {subsubsection}{Margin Infused Relaxed Algorithm (MIRA)}{32}{section*.17}}
\newlabel{eq:miraupdates}{{1.22}{32}{Margin Infused Relaxed Algorithm (MIRA)}{equation.1.4.22}{}}
\citation{Crammer2006}
\newlabel{alg:mira}{{3}{33}{Margin Infused Relaxed Algorithm (MIRA)}{algorithm.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces MIRA }}{33}{algorithm.3}}
\newlabel{eq:costfunc}{{1.25}{33}{Margin Infused Relaxed Algorithm (MIRA)}{equation.1.4.25}{}}
\citation{Shannon1948}
\citation{Jaynes1982}
\citation{Cover1991}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.4}Batch Discriminative Classifiers}{34}{subsection.1.4.4}}
\newlabel{s:me}{{1.4.4}{34}{Maximum Entropy Classifiers}{section*.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{Maximum Entropy Classifiers}{34}{section*.18}}
\newlabel{eq:loglinear}{{1.26}{34}{Maximum Entropy Classifiers}{equation.1.4.26}{}}
\citation{Nocedal1999}
\newlabel{eq:maxent}{{1.30}{35}{Maximum Entropy Classifiers}{equation.1.4.30}{}}
\@writefile{tdo}{\contentsline {todo}{NOTE-MA: A sigla SGD (stochastic gradient descent?) n„o foi definida; n„o e percebe o que È.}{35}{section*.19}}
\pgfsyspdfmark {pgfid11}{30699797}{8717795}
\pgfsyspdfmark {pgfid14}{37625178}{8707639}
\pgfsyspdfmark {pgfid15}{39100543}{8480885}
\newlabel{alg:maxent_gd}{{4}{36}{Maximum Entropy Classifiers}{algorithm.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Batch Gradient Descent for Maximum Entropy }}{36}{algorithm.4}}
\citation{Vapnik1995}
\citation{Crammer2002}
\citation{Schoelkopf2002}
\citation{ShaweTaylor2004}
\newlabel{alg:maxent_sgd}{{5}{37}{Maximum Entropy Classifiers}{algorithm.5}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces SGD for Maximum Entropy }}{37}{algorithm.5}}
\@writefile{toc}{\contentsline {subsubsection}{Support Vector Machines}{37}{section*.20}}
\newlabel{sec:svms}{{1.4.4}{37}{Support Vector Machines}{section*.20}{}}
\newlabel{eq:hingeloss}{{1.33}{37}{Support Vector Machines}{equation.1.4.33}{}}
\citation{ShalevShwartz2007ICML}
\newlabel{alg:svm_ssd}{{6}{38}{Support Vector Machines}{algorithm.6}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Stochastic Subgradient Descent for SVMs }}{38}{algorithm.6}}
\newlabel{eq:svm}{{1.34}{38}{Support Vector Machines}{equation.1.4.34}{}}
\newlabel{eq:svm2}{{1.35}{38}{Support Vector Machines}{equation.1.4.35}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Comparison}{39}{section.1.5}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Comparison among different algorithms.}}{39}{table.1.1}}
\newlabel{tab:comparison_lab1}{{1.1}{39}{Comparison among different algorithms}{table.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Final remarks}{39}{section.1.6}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Sequence Models}{40}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{day:seq}{{2}{40}{Sequence Models}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Notation}{40}{section.2.1}}
\newlabel{notation}{{2.1}{40}{Notation}{section.2.1}{}}
\newlabel{hmm}{{2.2}{40}{Hidden Markov Models}{section.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2} Hidden Markov Models}{40}{section.2.2}}
\newlabel{tab:hmm_notation}{{2.1}{41}{Notation}{Item.30}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces General notation used in this class}}{41}{table.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces HMM running example}}{42}{figure.2.1}}
\newlabel{fig:hmm}{{2.1}{42}{HMM running example}{figure.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces HMM notation}}{42}{table.2.2}}
\newlabel{tab:hmm-simple-notation}{{2.2}{42}{HMM notation}{table.2.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces HMM probability distributions}}{43}{table.2.3}}
\newlabel{tab:hmm-dist}{{2.3}{43}{HMM probability distributions}{table.2.3}{}}
\newlabel{eqn:hmm}{{2.1}{43}{Hidden Markov Models}{equation.2.2.1}{}}
\newlabel{eqn:hmm_ex}{{2.2}{43}{Hidden Markov Models}{equation.2.2.2}{}}
\newlabel{ml}{{2.3}{44}{Finding the Maximum Likelihood Parameters}{section.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3} Finding the Maximum Likelihood Parameters}{44}{section.2.3}}
\newlabel{eq::initialCounts}{{2.4}{44}{Finding the Maximum Likelihood Parameters}{equation.2.3.4}{}}
\newlabel{eq::transitionCounts}{{2.5}{44}{Finding the Maximum Likelihood Parameters}{equation.2.3.5}{}}
\newlabel{eq::finalCounts}{{2.6}{44}{Finding the Maximum Likelihood Parameters}{equation.2.3.6}{}}
\newlabel{eq::emissionCounts}{{2.7}{44}{Finding the Maximum Likelihood Parameters}{equation.2.3.7}{}}
\newlabel{decoding}{{2.4}{45}{Decoding a Sequence}{section.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4} Decoding a Sequence}{45}{section.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces HMM Trellis representation.}}{46}{figure.2.2}}
\newlabel{fig:trellis}{{2.2}{46}{HMM Trellis representation}{figure.2.2}{}}
\citation{rabiner}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Computing in log-domain}{47}{subsection.2.4.1}}
\newlabel{sec:logdomain}{{2.4.1}{47}{Computing in log-domain}{subsection.2.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Posterior Decoding}{47}{subsection.2.4.2}}
\newlabel{posterior}{{2.4.2}{47}{Posterior Decoding}{subsection.2.4.2}{}}
\newlabel{eq::posteriorDistribution}{{2.21}{48}{Posterior Decoding}{equation.2.4.21}{}}
\newlabel{eq::nodePosterior}{{2.22}{48}{Posterior Decoding}{equation.2.4.22}{}}
\newlabel{eq::edgePosterior}{{2.23}{48}{Posterior Decoding}{equation.2.4.23}{}}
\newlabel{likelihoood}{{2.24}{48}{Posterior Decoding}{equation.2.4.24}{}}
\newlabel{eq::forward}{{2.25}{48}{Posterior Decoding}{equation.2.4.25}{}}
\newlabel{forwardRecursion}{{2.27}{48}{Posterior Decoding}{equation.2.4.26}{}}
\newlabel{eq:forwardSum}{{2.29}{48}{Posterior Decoding}{equation.2.4.29}{}}
\newlabel{eq::backward}{{2.30}{48}{Posterior Decoding}{equation.2.4.30}{}}
\newlabel{alg:fb}{{7}{49}{Posterior Decoding}{algorithm.7}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces Forward-Backward algorithm }}{49}{algorithm.7}}
\newlabel{backwardRecursion}{{2.32}{49}{Posterior Decoding}{equation.2.4.31}{}}
\newlabel{eq:backwardSum}{{2.34}{49}{Posterior Decoding}{equation.2.4.34}{}}
\newlabel{eq::fbsanity}{{2.35}{49}{Posterior Decoding}{equation.2.4.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Posterior Illustration.}}{50}{figure.2.3}}
\newlabel{fig:posteriors}{{2.3}{50}{Posterior Illustration}{figure.2.3}{}}
\newlabel{eq::nodePosterior2}{{2.36}{50}{Posterior Decoding}{equation.2.4.36}{}}
\newlabel{eq::edgePosterior2}{{2.37}{50}{Posterior Decoding}{equation.2.4.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Viterbi Decoding}{51}{subsection.2.4.3}}
\newlabel{viterbi}{{2.4.3}{51}{Viterbi Decoding}{subsection.2.4.3}{}}
\newlabel{eq::viterbi}{{2.40}{51}{Viterbi Decoding}{equation.2.4.40}{}}
\newlabel{alg:viterbi}{{8}{52}{Viterbi Decoding}{algorithm.8}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {8}{\ignorespaces Viterbi algorithm }}{52}{algorithm.8}}
\newlabel{viterbiRecursion}{{2.42}{52}{Viterbi Decoding}{equation.2.4.41}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Assignment}{52}{section.2.5}}
\citation{pennTreeBank}
\citation{pennTreeBank}
\@writefile{tdo}{\contentsline {todo}{NOTA-MA: O segundo dataset ainda n√£o foi definido.}{53}{section*.22}}
\pgfsyspdfmark {pgfid16}{27276799}{47101442}
\pgfsyspdfmark {pgfid19}{37625178}{47091286}
\pgfsyspdfmark {pgfid20}{39100543}{46864532}
\newlabel{pos-tagging}{{2.6}{53}{Part-of-Speech Tagging (POS)}{section.2.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6} Part-of-Speech Tagging (POS)}{53}{section.2.6}}
\@writefile{tdo}{\contentsline {todo}{NOTA-MA: codigo usa 1000 em toodos os conjuntos... }{53}{section*.23}}
\pgfsyspdfmark {pgfid21}{33583269}{19769977}
\pgfsyspdfmark {pgfid24}{37625178}{19766211}
\pgfsyspdfmark {pgfid25}{39100543}{19539457}
\@writefile{tdo}{\contentsline {todo}{NOTA-MA: a funcao viterbi\_decode\_corpus imprime muita infroma√ßao.}{53}{section*.24}}
\pgfsyspdfmark {pgfid26}{10276323}{5447222}
\pgfsyspdfmark {pgfid29}{37625178}{5437066}
\pgfsyspdfmark {pgfid30}{39100543}{5210312}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces  Transition probabilities of the trained model. Each column is the previous state and row is the current state. Note the high probability of having Noun after Adjective, or of having Verb after Noun, as expected.}}{54}{figure.2.4}}
\newlabel{fig:transProbs}{{2.4}{54}{Transition probabilities of the trained model. Each column is the previous state and row is the current state. Note the high probability of having Noun after Adjective, or of having Verb after Noun, as expected}{figure.2.4}{}}
\citation{schutze1995distributional}
\citation{merialdo1994tet}
\citation{clark03combining}
\citation{klein2004acl}
\citation{smith2006annealing}
\citation{klein2004acl}
\citation{brown94mathematic}
\citation{charniak2009works}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces  Confusion Matrix for the previous example. Predicted tags are columns and the true tags correspond to the constituents of each column.}}{55}{figure.2.5}}
\newlabel{fig:cm_uns}{{2.5}{55}{Confusion Matrix for the previous example. Predicted tags are columns and the true tags correspond to the constituents of each column}{figure.2.5}{}}
\newlabel{unsupervised}{{2.7}{55}{Unsupervised Learning of HMMs}{section.2.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7} Unsupervised Learning of HMMs}{55}{section.2.7}}
\citation{Neal1998}
\newlabel{em}{{2.7.1}{56}{Expectation Maximization Algorithm}{subsection.2.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}Expectation Maximization Algorithm}{56}{subsection.2.7.1}}
\newlabel{loglikelihoood}{{2.47}{56}{Expectation Maximization Algorithm}{equation.2.7.47}{}}
\newlabel{eq::initialCountsPost}{{2.48}{56}{Expectation Maximization Algorithm}{equation.2.7.48}{}}
\newlabel{eq::transitionCountsPost}{{2.49}{56}{Expectation Maximization Algorithm}{equation.2.7.49}{}}
\newlabel{eq::finalCountsPost}{{2.50}{56}{Expectation Maximization Algorithm}{equation.2.7.50}{}}
\newlabel{eq::emissionCountsPost}{{2.51}{56}{Expectation Maximization Algorithm}{equation.2.7.51}{}}
\newlabel{alg::em}{{9}{57}{Expectation Maximization Algorithm}{algorithm.9}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {9}{\ignorespaces EM algorithm}}{57}{algorithm.9}}
\citation{Reichart09}
\citation{haghighi2006naacl}
\citation{Meila07}
\citation{RosenbergH07}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces  Confusion Matrix example. Each cluster is a column. The best tag in each column is represented under the column (1-many) mapping. Each color represents a true Pos Tag.}}{58}{figure.2.6}}
\newlabel{fig:cm_uns}{{2.6}{58}{Confusion Matrix example. Each cluster is a column. The best tag in each column is represented under the column (1-many) mapping. Each color represents a true Pos Tag}{figure.2.6}{}}
\newlabel{posi}{{2.7.2}{58}{Part of Speech Induction}{subsection.2.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.2}Part of Speech Induction}{58}{subsection.2.7.2}}
\citation{JoaoThesis}
\citation{bergkirkpatrick2010naacl}
\citation{das-petrov:2011:ACL-HLT2011}
\citation{johnson2007dtf}
\citation{graca2009nips}
\citation{collins2002discriminative}
\citation{lafferty2001conditional}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Learning Structured Predictors}{60}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{day:seq_disc}{{3}{60}{Learning Structured Predictors}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Classification vs Sequential Classification}{60}{section.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Summary of the methods used for classification and sequential classification covered in this guide.}}{60}{table.3.1}}
\newlabel{disc_seq_summary}{{3.1}{60}{Summary of the methods used for classification and sequential classification covered in this guide}{table.3.1}{}}
\newlabel{eq::struc_pred}{{3.1}{60}{Classification vs Sequential Classification}{equation.3.1.1}{}}
\@writefile{tdo}{\contentsline {todo}{nao percebo muito bem esta nota em italico... Ou é "in fact, .. will touch" ou "However, ... will touch."}{60}{section*.26}}
\pgfsyspdfmark {pgfid31}{22018725}{6798714}
\pgfsyspdfmark {pgfid34}{37625178}{6794948}
\pgfsyspdfmark {pgfid35}{39100543}{6568194}
\newlabel{eq::struc_pred_decompose}{{3.2}{61}{Classification vs Sequential Classification}{equation.3.1.2}{}}
\newlabel{seq::features}{{3.2}{61}{Features}{section.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2} Features}{61}{section.3.2}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces  {\tt  IDFeatures} feature set. This set replicates the features used by the HMM model.}}{61}{table.3.2}}
\newlabel{id-features}{{3.2}{61}{{\tt IDFeatures} feature set. This set replicates the features used by the HMM model}{table.3.2}{}}
\@writefile{tdo}{\contentsline {todo}{NOTA-MA: Nos HMM (de order superior) tb, certo?}{61}{section*.27}}
\pgfsyspdfmark {pgfid36}{23142212}{19259050}
\pgfsyspdfmark {pgfid39}{37625178}{19255284}
\pgfsyspdfmark {pgfid40}{39100543}{19028530}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces  {\tt  Extended} feature set. Some features in this set could not be included in the HMM model. The features included in the bottom row are all considered emission features for the purpose of our implementation, since they all depend on $i$, $x$ and $y_i$.}}{62}{table.3.3}}
\newlabel{ex-features}{{3.3}{62}{{\tt Extended} feature set. Some features in this set could not be included in the HMM model. The features included in the bottom row are all considered emission features for the purpose of our implementation, since they all depend on $i$, $x$ and $y_i$}{table.3.3}{}}
\newlabel{dis_node_potentials}{{3.6}{62}{Features}{equation.3.2.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Discriminative Sequential Classifiers}{62}{section.3.3}}
\newlabel{eq:disc_formula}{{3.7}{62}{Discriminative Sequential Classifiers}{equation.3.3.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Training}{62}{subsection.3.3.1}}
\citation{lafferty2001conditional}
\citation{mccallum2000maximum}
\citation{Bottou1991}
\citation{lafferty2001conditional}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Decoding}{63}{subsection.3.3.2}}
\newlabel{s:crf}{{3.4}{63}{Conditional Random Fields}{section.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Conditional Random Fields}{63}{section.3.4}}
\newlabel{exer:crf1}{{3.1}{63}{Conditional Random Fields}{exercise.3.1}{}}
\newlabel{alg:crf_online}{{10}{64}{Conditional Random Fields}{algorithm.10}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {10}{\ignorespaces SGD for Conditional Random Fields }}{64}{algorithm.10}}
\@writefile{tdo}{\contentsline {todo}{Acho que nao vale a pena ter o output de tantas epocas}{64}{section*.28}}
\pgfsyspdfmark {pgfid41}{3729359}{9435104}
\pgfsyspdfmark {pgfid44}{37625178}{9424948}
\pgfsyspdfmark {pgfid45}{39100543}{9198194}
\@writefile{tdo}{\contentsline {todo}{NOTA-MA: a) O development set n√£o √© usado. b) √â suposto os conjuntos de teste e desenvolvimento terem 1000 frases (e n√£o 200)?}{64}{section*.29}}
\pgfsyspdfmark {pgfid46}{29291979}{7783592}
\pgfsyspdfmark {pgfid49}{37625178}{4774324}
\pgfsyspdfmark {pgfid50}{39100543}{4547570}
\newlabel{exer:crf2}{{3.2}{65}{Conditional Random Fields}{exercise.3.2}{}}
\citation{collins2002discriminative}
\newlabel{s:spercetron}{{3.5}{66}{Structured Perceptron}{section.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Structured Perceptron}{66}{section.3.5}}
\newlabel{alg:structured-perceptron}{{11}{66}{Structured Perceptron}{algorithm.11}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {11}{\ignorespaces Averaged Structured perceptron }}{66}{algorithm.11}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Assignment}{66}{section.3.6}}
\newlabel{exer:strucperc1}{{3.3}{66}{Assignment}{exercise.3.3}{}}
\@writefile{tdo}{\contentsline {todo}{NOTA-MA: ainda tenho de terminar o codigo.}{68}{section*.30}}
\pgfsyspdfmark {pgfid51}{13739870}{21293368}
\pgfsyspdfmark {pgfid54}{37625178}{21283212}
\pgfsyspdfmark {pgfid55}{39100543}{21056458}
\citation{Hopcroft1979}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Syntax and Parsing}{69}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Phrase-based Parsing}{69}{section.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Context Free Grammars}{69}{subsection.4.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Ambiguity}{70}{subsection.4.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Probabilistic Context-Free Grammars}{71}{subsection.4.1.3}}
\citation{Manning1999}
\newlabel{alg:cky}{{12}{72}{The CKY Parsing Algorithm}{algorithm.12}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {12}{\ignorespaces CKY algorithm }}{72}{algorithm.12}}
\newlabel{eq:jointcfg}{{4.1}{72}{Probabilistic Context-Free Grammars}{equation.4.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}The CKY Parsing Algorithm}{72}{subsection.4.1.4}}
\newlabel{exer:cky}{{4.1}{72}{The CKY Parsing Algorithm}{exercise.4.1}{}}
\citation{Manning1999}
\citation{Klein2002}
\citation{Smith2005}
\citation{Cohen2008}
\citation{Johnson1998}
\citation{Klein2003}
\citation{Magerman1995}
\citation{Charniak1997}
\citation{Collins1999}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.5}Learning the Grammar}{73}{subsection.4.1.5}}
\newlabel{exer:treebank}{{4.2}{73}{Learning the Grammar}{exercise.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.6}Model Refinements}{73}{subsection.4.1.6}}
\citation{Taskar2004}
\citation{Finkel2008}
\citation{Petrov2007NAACL}
\citation{Petrov2008NIPS}
\citation{Petrov2008EMNLP}
\citation{Charniak2006}
\citation{Ratnaparkhi1999}
\citation{Henderson2003}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A lexicalized parse tree for the sentence \emph  {She enjoys the Summer school}.}}{74}{figure.4.1}}
\newlabel{fig:lexicalizedtree}{{4.1}{74}{A lexicalized parse tree for the sentence \emph {She enjoys the Summer school}}{figure.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Dependency Parsing}{74}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Motivation}{74}{subsection.4.2.1}}
\citation{Chomsky1965}
\citation{Tesniere1959}
\citation{Hudson1984}
\citation{Melcuk1988}
\citation{Covington1990}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces A dependency tree for the sentence \emph  {She enjoys the Summer school}. Note the additional dummy root symbol (*) which is included for convenience.}}{75}{figure.4.2}}
\newlabel{fig:deptree_proj}{{4.2}{75}{A dependency tree for the sentence \emph {She enjoys the Summer school}. Note the additional dummy root symbol (*) which is included for convenience}{figure.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Projective and Non-projective Parsing}{75}{subsection.4.2.2}}
\citation{Eisner1996}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces A non-projective parse tree.}}{76}{figure.4.3}}
\newlabel{fig:deptree_nonproj}{{4.3}{76}{A non-projective parse tree}{figure.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Algorithms for Projective Dependency Parsing}{76}{subsection.4.2.3}}
\@writefile{tdo}{\contentsline {todo}{Skip this exercise and write down an optional one at the end asking to implement Eisner's algorithm given some pseudo-code and evaluate it on English data.}{76}{section*.31}}
\pgfsyspdfmark {pgfid56}{4712399}{13343627}
\pgfsyspdfmark {pgfid59}{37625178}{13339861}
\pgfsyspdfmark {pgfid60}{39100543}{13113107}
\citation{Chu1965}
\citation{Edmonds1967}
\citation{Tarjan1977}
\citation{McDonald2005b}
\citation{Tutte1984}
\citation{DSmithSmith2007}
\citation{Koo2007}
\citation{McDonald2007}
\citation{conll06st}
\citation{Surdeanu2008}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Algorithms for Non-Projective Dependency Parsing}{77}{subsection.4.2.4}}
\citation{Eisner1999}
\citation{Carreras2007}
\citation{McDonald2007}
\citation{McDonald2006CoNLL}
\citation{DSmith2008}
\citation{Martins2009ACL}
\citation{Koo2010EMNLP}
\citation{Koo2010}
\citation{Martins2013ACL}
\newlabel{alg:eisner}{{13}{80}{Algorithms for Non-Projective Dependency Parsing}{algorithm.13}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {13}{\ignorespaces Eisner's algorithm for first-order projective dependency parsing}}{80}{algorithm.13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Model Refinements}{80}{subsection.4.2.5}}
\citation{Nivre2006CoNLL}
\citation{Huang2010}
\citation{Nivre2009}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.6}External Links}{81}{subsection.4.2.6}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Big Data I: Introduction}{82}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{82}{section.5.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Useful Information}{82}{section.5.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}MapReduce for Word Count}{83}{section.5.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Running Word Count locally}{83}{subsection.5.3.1}}
\@writefile{tdo}{\contentsline {todo}{No final temos de ver se e esta a pasta.}{84}{section*.34}}
\pgfsyspdfmark {pgfid61}{46295097}{42823594}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Running Word Count on Amazon EC2}{84}{subsection.5.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Using Na\"{i}ve Bayes for Language Detection}{84}{section.5.4}}
\newlabel{eq:NBformula}{{5.2}{85}{Using Na\"{i}ve Bayes for Language Detection}{equation.5.4.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Assignment}{85}{section.5.5}}
\bibstyle{apalike}
\bibdata{guide}
