\relax 
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\citation{PER-GRA:2007}
\@writefile{toc}{\contentsline {chapter}{\numberline {0}Basic Tutorials}{1}{chapter.0}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Python}{1}{section.0.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.1}Running Python}{1}{subsection.0.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{Help and Documentation}{1}{section*.1}}
\@writefile{toc}{\contentsline {subsubsection}{Profiling}{2}{section*.2}}
\@writefile{toc}{\contentsline {subsubsection}{Exiting}{2}{section*.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.2}Python by Example}{2}{subsection.0.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{Data Structures}{2}{section*.4}}
\@writefile{toc}{\contentsline {subsubsection}{Loops}{2}{section*.5}}
\@writefile{toc}{\contentsline {subsubsection}{Control Flow}{3}{section*.6}}
\@writefile{toc}{\contentsline {subsubsection}{Functions}{3}{section*.7}}
\@writefile{toc}{\contentsline {subsubsection}{Indentation}{3}{section*.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.3}Debugging in Python}{4}{subsection.0.1.3}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Basic pdb/ipdb commands, parentheses indicates abbreviation}}{4}{table.0.1}}
\newlabel{tb::pdbbasiccommands}{{1}{4}{Basic pdb/ipdb commands, parentheses indicates abbreviation\relax }{table.0.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.4}Plotting in Python - Matplotlib}{5}{subsection.0.1.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.5}Numpy}{6}{subsection.0.1.5}}
\@writefile{toc}{\contentsline {subsubsection}{Multidimensional Arrays}{6}{section*.9}}
\@writefile{toc}{\contentsline {subsubsection}{Mathematical Operations}{6}{section*.10}}
\@writefile{toc}{\contentsline {section}{\numberline {0.2}Essential Linear Algebra}{7}{section.0.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.1}Notation}{7}{subsection.0.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.2}Some Matrix Operations and Properties}{8}{subsection.0.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.3}Norms}{10}{subsection.0.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.4}Linear Independence and Rank}{10}{subsection.0.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {0.3}Probability Theory}{10}{section.0.3}}
\newlabel{eq:condprob}{{2}{11}{Probability Theory\relax }{equation.0.3.2}{}}
\newlabel{eq:totprob}{{3}{11}{Probability Theory\relax }{equation.0.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.1}Probability distribution functions}{12}{subsection.0.3.1}}
\newlabel{bernoulli}{{0.3.2}{12}{Bernoulli\relax }{subsection.0.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.2} Bernoulli}{12}{subsection.0.3.2}}
\newlabel{bernoulli-eq}{{0.3.2}{12}{Bernoulli\relax }{subsection.0.3.2}{}}
\newlabel{binomial}{{0.3.3}{12}{Binomial\relax }{subsection.0.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.3} Binomial}{12}{subsection.0.3.3}}
\newlabel{categorical}{{0.3.4}{13}{Categorical\relax }{subsection.0.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.4} Categorical}{13}{subsection.0.3.4}}
\newlabel{multinomial}{{0.3.5}{13}{Multinomial\relax }{subsection.0.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.5} Multinomial}{13}{subsection.0.3.5}}
\newlabel{gaussian}{{0.3.6}{13}{Gaussian Distribution\relax }{subsection.0.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.6} Gaussian Distribution}{13}{subsection.0.3.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Normal density for two sets of parameter values.}}{13}{figure.0.1}}
\newlabel{fig:normaldist}{{1}{13}{Normal density for two sets of parameter values}{figure.0.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.7}Maximum Likelihood Estimation}{13}{subsection.0.3.7}}
\citation{Nocedal1999}
\citation{bertsekas1995np}
\citation{boyd2004convex}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.8}Conjugate Priors}{15}{subsection.0.3.8}}
\@writefile{toc}{\contentsline {section}{\numberline {0.4}Numerical optimization}{15}{section.0.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.4.1}Convex Functions}{15}{subsection.0.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Illustration of a convex function. The line segment between any two points on the graph lies entirely above the curve.}}{16}{figure.0.2}}
\newlabel{fig:convexfn}{{2}{16}{Illustration of a convex function. The line segment between any two points on the graph lies entirely above the curve}{figure.0.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Illustration of a non-convex function. Note the line segment intersecting the curve. }}{16}{figure.0.3}}
\newlabel{fig:nonconvexfn}{{3}{16}{Illustration of a non-convex function. Note the line segment intersecting the curve}{figure.0.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.4.2}Derivative and Gradient}{16}{subsection.0.4.2}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Some derivative examples}}{17}{table.0.2}}
\newlabel{tb::derivatives}{{2}{17}{Some derivative examples\relax }{table.0.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.4.3}Gradient Based Methods}{17}{subsection.0.4.3}}
\citation{Nocedal1999}
\citation{Nocedal1999}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Illustration of the gradient of the function $f(x^2)$ at three different points $x = [-2,0.2]$. Note that at point $x = 0$ the gradient is zero which corresponds to the minimum of the function.}}{18}{figure.0.4}}
\newlabel{fig:tangents}{{4}{18}{Illustration of the gradient of the function $f(x^2)$ at three different points $x = [-2,0.2]$. Note that at point $x = 0$ the gradient is zero which corresponds to the minimum of the function}{figure.0.4}{}}
\newlabel{alg:graddescent}{{1}{18}{Gradient Based Methods\relax }{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gradient Descent}}{18}{algorithm.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Illustration of gradient descent. The blue circles correspond to contours of the function (each blue circle is a set of points which have the same function value), while the red lines correspond to steps taken in the negative gradient direction.}}{19}{figure.0.5}}
\newlabel{fig:graddescent}{{5}{19}{Illustration of gradient descent. The blue circles correspond to contours of the function (each blue circle is a set of points which have the same function value), while the red lines correspond to steps taken in the negative gradient direction}{figure.0.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Example of running gradient descent starting on point $x_0 = -8$ for function $f(x) = (x+2)^2 - 16 \qopname  \relax o{exp}\left ( -(x-2)^2 \right )$. The function is represented in blue, while the points of the minimizing sequence are displayed as green plus signs.}}{21}{figure.0.6}}
\newlabel{fig:gradex1}{{6}{21}{Example of running gradient descent starting on point $x_0 = -8$ for function $f(x) = (x+2)^2 - 16 \exp \left ( -(x-2)^2 \right )$. The function is represented in blue, while the points of the minimizing sequence are displayed as green plus signs}{figure.0.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Example of running gradient descent starting on point $x_0 = 8$ for function $f(x) = (x+2)^2 - 16 \qopname  \relax o{exp}\left ( -(x-2)^2 \right )$. The function is represented in blue, while the points of the minimizing sequence are displayed as green plus signs.}}{22}{figure.0.7}}
\newlabel{fig:gradex2}{{7}{22}{Example of running gradient descent starting on point $x_0 = 8$ for function $f(x) = (x+2)^2 - 16 \exp \left ( -(x-2)^2 \right )$. The function is represented in blue, while the points of the minimizing sequence are displayed as green plus signs}{figure.0.7}{}}
\citation{Mitchell1997}
\citation{Duda2001}
\citation{Schoelkopf2002}
\citation{Joachims2002}
\citation{Bishop2006}
\citation{Manning2008}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Classification}{23}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{day:classification}{{1}{23}{Classification\relax }{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Notation}{23}{section.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces  Example of a dataset. The input set consists in points in the real plane, $\EuScript  {X} = \PazoBB  {R}^2$, and the output set consists of two classes (Red and Blue). Training points are represented as squares, while test points are represented as circles.}}{24}{figure.1.1}}
\newlabel{simpleDataSet}{{1.1}{24}{Example of a dataset. The input set consists in points in the real plane, $\mathcal {X} = \mathbb {R}^2$, and the output set consists of two classes (Red and Blue). Training points are represented as squares, while test points are represented as circles}{figure.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces  Example of a dataset together with the corresponding Bayes optimal decision boundary. The input set consists in points in the real plane, $\EuScript  {X} = \EuScript  {R^2}$, and the output set consists of two classes (Red and Blue). Training points are represented as squares, while test points are represented as circles.}}{25}{figure.1.2}}
\newlabel{simpleDataSet_bo}{{1.2}{25}{Example of a dataset together with the corresponding Bayes optimal decision boundary. The input set consists in points in the real plane, $\mathcal {X} = \mathcal {R^2}$, and the output set consists of two classes (Red and Blue). Training points are represented as squares, while test points are represented as circles}{figure.1.2}{}}
\newlabel{s::naiveBayes}{{1.2}{25}{Generative Classifiers: Na\"{i}ve Bayes\relax }{section.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Generative Classifiers: Na\"{i}ve Bayes}{25}{section.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Example: 2-D Gaussians}{26}{subsection.1.2.1}}
\newlabel{exer:simplenb}{{1.1}{27}{Example: 2-D Gaussians\relax }{exercise.1.1}{}}
\citation{Manning2008}
\citation{McCallum1998}
\citation{blitzer2007biographies}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Example: Multinomial Model for Document Classification}{28}{subsection.1.2.2}}
\newlabel{eq:mlemultinomial}{{1.10}{28}{Example: Multinomial Model for Document Classification\relax }{equation.1.2.10}{}}
\citation{Manning1999}
\citation{Manning2008}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Features and Linear Classifiers}{29}{section.1.3}}
\newlabel{sec:linearclass}{{1.3}{29}{Features and Linear Classifiers\relax }{section.1.3}{}}
\citation{Rosenblatt1958}
\newlabel{eq:jointfeatsimple}{{1.17}{30}{Features and Linear Classifiers\relax }{equation.1.3.17}{}}
\citation{Crammer2002}
\citation{Crammer2006}
\newlabel{alg:perceptron}{{2}{31}{Perceptron\relax }{algorithm.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces  Averaged perceptron}}{31}{algorithm.2}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4} Online Algorithms: Perceptron and MIRA}{31}{section.1.4}}
\newlabel{s:perceptron}{{1.4.1}{31}{Perceptron\relax }{subsection.1.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1} Perceptron}{31}{subsection.1.4.1}}
\citation{Crammer2006}
\newlabel{alg:mira}{{3}{32}{Margin Infused Relaxed Algorithm (MIRA)\relax }{algorithm.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces MIRA }}{32}{algorithm.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Margin Infused Relaxed Algorithm (MIRA)}{32}{subsection.1.4.2}}
\newlabel{eq:miraupdates}{{1.23}{32}{Margin Infused Relaxed Algorithm (MIRA)\relax }{equation.1.4.23}{}}
\newlabel{eq:costfunc}{{1.26}{32}{Margin Infused Relaxed Algorithm (MIRA)\relax }{equation.1.4.26}{}}
\citation{Shannon1948}
\citation{Jaynes1982}
\citation{Cover1991}
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Discriminative Classifiers: Maximum Entropy and Support Vector Machines}{33}{section.1.5}}
\newlabel{s:me}{{1.5.1}{33}{Maximum Entropy Classifiers\relax }{subsection.1.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}Maximum Entropy Classifiers}{33}{subsection.1.5.1}}
\newlabel{eq:loglinear}{{1.27}{33}{Maximum Entropy Classifiers\relax }{equation.1.5.27}{}}
\citation{Nocedal1999}
\newlabel{eq:maxent}{{1.31}{34}{Maximum Entropy Classifiers\relax }{equation.1.5.31}{}}
\newlabel{alg:maxent_gd}{{4}{35}{Maximum Entropy Classifiers\relax }{algorithm.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Batch Gradient Descent for Maximum Entropy }}{35}{algorithm.4}}
\citation{Vapnik1995}
\citation{Crammer2002}
\citation{Schoelkopf2002}
\citation{ShaweTaylor2004}
\newlabel{alg:maxent_sgd}{{5}{36}{Maximum Entropy Classifiers\relax }{algorithm.5}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces SGD for Maximum Entropy }}{36}{algorithm.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.2}Support Vector Machines}{36}{subsection.1.5.2}}
\newlabel{sec:svms}{{1.5.2}{36}{Support Vector Machines\relax }{subsection.1.5.2}{}}
\citation{ShalevShwartz2007ICML}
\newlabel{alg:svm_ssd}{{6}{37}{Support Vector Machines\relax }{algorithm.6}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Stochastic Subgradient Descent for SVMs }}{37}{algorithm.6}}
\newlabel{eq:hingeloss}{{1.34}{37}{Support Vector Machines\relax }{equation.1.5.34}{}}
\newlabel{eq:svm}{{1.35}{37}{Support Vector Machines\relax }{equation.1.5.35}{}}
\newlabel{eq:svm2}{{1.36}{37}{Support Vector Machines\relax }{equation.1.5.36}{}}
\newlabel{tab:comparison_lab1}{{1.6}{38}{Comparison\relax }{section.1.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Comparison among different models.}}{38}{table.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Comparison}{38}{section.1.6}}
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Final remarks}{38}{section.1.7}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Sequence Models}{39}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{day:seq}{{2}{39}{Sequence Models\relax }{chapter.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces General notation used in this class}}{39}{table.2.1}}
\newlabel{hmm}{{2.1}{39}{Hidden Markov Models\relax }{section.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1} Hidden Markov Models}{39}{section.2.1}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces HMM notation}}{40}{table.2.2}}
\newlabel{tab:hmm-simple-notation}{{2.2}{40}{HMM notation\relax }{table.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces HMM running example}}{41}{figure.2.1}}
\newlabel{fig:hmm}{{2.1}{41}{HMM running example\relax }{figure.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces HMM probability distributions}}{41}{table.2.3}}
\newlabel{tab:hmm-dist}{{2.3}{41}{HMM probability distributions\relax }{table.2.3}{}}
\newlabel{eqn:hmm}{{2.1}{41}{Hidden Markov Models\relax }{equation.2.1.1}{}}
\newlabel{eqn:hmm_ex}{{2.2}{41}{Hidden Markov Models\relax }{equation.2.1.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces HMM multinomial parametrization}}{42}{table.2.4}}
\newlabel{tt:mult-params}{{2.4}{42}{HMM multinomial parametrization\relax }{table.2.4}{}}
\newlabel{ml}{{2.2}{42}{Finding the Maximum Likelihood Parameters\relax }{section.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2} Finding the Maximum Likelihood Parameters}{42}{section.2.2}}
\newlabel{eq::initialCounts}{{2.5}{42}{Finding the Maximum Likelihood Parameters\relax }{equation.2.2.5}{}}
\newlabel{eq::transitionCounts}{{2.6}{42}{Finding the Maximum Likelihood Parameters\relax }{equation.2.2.6}{}}
\newlabel{eq::stateCounts}{{2.7}{42}{Finding the Maximum Likelihood Parameters\relax }{equation.2.2.7}{}}
\newlabel{decoding}{{2.3}{43}{Finding the most likely sequence - Decoding\relax }{section.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3} Finding the most likely sequence - Decoding}{43}{section.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces HMM Trellis representation.}}{44}{figure.2.2}}
\newlabel{fig:trellis}{{2.2}{44}{HMM Trellis representation}{figure.2.2}{}}
\newlabel{eqn:hmm_ex_treelis}{{2.13}{44}{Finding the most likely sequence - Decoding\relax }{equation.2.3.13}{}}
\newlabel{eq:nodes1}{{2.14}{44}{Finding the most likely sequence - Decoding\relax }{equation.2.3.14}{}}
\newlabel{eq:nodes2}{{2.15}{44}{Finding the most likely sequence - Decoding\relax }{equation.2.3.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Posterior Decoding}{44}{subsection.2.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Forward backward example.}}{45}{figure.2.3}}
\newlabel{fig:fb}{{2.3}{45}{Forward backward example}{figure.2.3}{}}
\newlabel{eq::posteriorDistribution}{{2.17}{45}{Posterior Decoding\relax }{equation.2.3.17}{}}
\newlabel{eq::nodePosterior}{{2.18}{45}{Posterior Decoding\relax }{equation.2.3.18}{}}
\newlabel{eq::edgePosterior}{{2.19}{45}{Posterior Decoding\relax }{equation.2.3.19}{}}
\newlabel{likelihoood}{{2.20}{45}{Posterior Decoding\relax }{equation.2.3.20}{}}
\newlabel{eq::forward}{{2.21}{45}{Posterior Decoding\relax }{equation.2.3.21}{}}
\newlabel{forwardRecursion}{{2.23}{45}{Posterior Decoding\relax }{equation.2.3.22}{}}
\citation{rabiner}
\newlabel{alg:fb}{{7}{46}{Posterior Decoding\relax }{algorithm.7}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces Forward Backward algorithm }}{46}{algorithm.7}}
\newlabel{eq:forwardSum}{{2.24}{46}{Posterior Decoding\relax }{equation.2.3.24}{}}
\newlabel{eq::backward}{{2.25}{46}{Posterior Decoding\relax }{equation.2.3.25}{}}
\newlabel{eq::fbsanity}{{2.28}{46}{Posterior Decoding\relax }{equation.2.3.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Posterior Illustration.}}{47}{figure.2.4}}
\newlabel{fig:posteriors}{{2.4}{47}{Posterior Illustration}{figure.2.4}{}}
\newlabel{eq::nodePosterior2}{{2.29}{47}{Posterior Decoding\relax }{equation.2.3.29}{}}
\newlabel{eq::edgePosterior2}{{2.30}{47}{Posterior Decoding\relax }{equation.2.3.30}{}}
\newlabel{alg:pd}{{8}{48}{Posterior Decoding\relax }{algorithm.8}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {8}{\ignorespaces Posterior Decoding algorithm }}{48}{algorithm.8}}
\newlabel{alg:viterbi}{{9}{49}{Viterbi Decoding\relax }{algorithm.9}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {9}{\ignorespaces Viterbi algorithm }}{49}{algorithm.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Viterbi Decoding}{49}{subsection.2.3.2}}
\newlabel{eq::viterbi}{{2.32}{49}{Viterbi Decoding\relax }{equation.2.3.32}{}}
\newlabel{viterbiRecursion}{{2.34}{49}{Viterbi Decoding\relax }{equation.2.3.33}{}}
\citation{pennTreeBank}
\newlabel{pos-tagging}{{2.4}{50}{Part-of-Speech Tagging (POS)\relax }{section.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4} Part-of-Speech Tagging (POS)}{50}{section.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces  Transition probabilities of the trained model. Each column is previous state and row is current state. Note the high probability of having Noun after Adjective, or of having Verb after Noun, as expected.}}{51}{figure.2.5}}
\newlabel{fig:transProbs}{{2.5}{51}{Transition probabilities of the trained model. Each column is previous state and row is current state. Note the high probability of having Noun after Adjective, or of having Verb after Noun, as expected}{figure.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces  Confusion Matrix for the previous example. Predict tags are columns and the true tags corresponds to the constituents of each column.}}{52}{figure.2.6}}
\newlabel{fig:cm_uns}{{2.6}{52}{Confusion Matrix for the previous example. Predict tags are columns and the true tags corresponds to the constituents of each column}{figure.2.6}{}}
\citation{collins2002discriminative}
\citation{lafferty2001conditional}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Learning Structured Predictors}{53}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{day:seq_disc}{{3}{53}{Learning Structured Predictors\relax }{chapter.3}{}}
\newlabel{eq::struc_pred}{{3.1}{53}{Learning Structured Predictors\relax }{equation.3.0.1}{}}
\newlabel{eq::struc_pred_decompose}{{3.2}{53}{Learning Structured Predictors\relax }{equation.3.0.2}{}}
\newlabel{seq::features}{{3.1}{53}{Feature Extraction\relax }{section.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1} Feature Extraction}{53}{section.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Summary of the methods that we will be covering this lecture.}}{53}{table.3.1}}
\newlabel{disc_seq_summary}{{3.1}{53}{Summary of the methods that we will be covering this lecture}{table.3.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces  IDFeatures feature set. This set replicates the features used by the HMM model.}}{54}{table.3.2}}
\newlabel{id-features}{{3.2}{54}{IDFeatures feature set. This set replicates the features used by the HMM model}{table.3.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces  Extended feature set. Some features in this set could not be included in the HMM model.}}{54}{table.3.3}}
\newlabel{ex-features}{{3.3}{54}{Extended feature set. Some features in this set could not be included in the HMM model}{table.3.3}{}}
\newlabel{dis_node_potentials}{{3.3}{54}{Feature Extraction\relax }{equation.3.1.3}{}}
\newlabel{dis_edge_potentials}{{3.4}{54}{Feature Extraction\relax }{equation.3.1.4}{}}
\newlabel{eq:disc_formula}{{3.5}{54}{Feature Extraction\relax }{equation.3.1.5}{}}
\citation{collins2002discriminative}
\newlabel{alg:structured-perceptron}{{10}{55}{Structured Perceptron\relax }{algorithm.10}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {10}{\ignorespaces Averaged Structured perceptron }}{55}{algorithm.10}}
\newlabel{s:spercetron}{{3.2}{55}{Structured Perceptron\relax }{section.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Structured Perceptron}{55}{section.3.2}}
\newlabel{exer:strucperc1}{{3.1}{55}{Structured Perceptron\relax }{exercise.3.1}{}}
\newlabel{exer:strucperc2}{{3.2}{56}{Structured Perceptron\relax }{exercise.3.2}{}}
\citation{lafferty2001conditional}
\citation{mccallum2000maximum}
\citation{Bottou1991}
\citation{lafferty2001conditional}
\newlabel{s:crf}{{3.3}{57}{Conditional Random Fields\relax }{section.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Conditional Random Fields}{57}{section.3.3}}
\newlabel{alg:crf_batch}{{11}{58}{Conditional Random Fields\relax }{algorithm.11}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {11}{\ignorespaces Batch Gradient Descent for Conditional Random Fields }}{58}{algorithm.11}}
\citation{Hopcroft1979}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Syntax and Parsing}{59}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Phrase-based Parsing}{59}{section.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Context Free Grammars}{59}{subsection.4.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Ambiguity}{60}{subsection.4.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Probabilistic Context-Free Grammars}{61}{subsection.4.1.3}}
\citation{Manning1999}
\newlabel{alg:cky}{{12}{62}{The CKY Parsing Algorithm\relax }{algorithm.12}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {12}{\ignorespaces CKY algorithm }}{62}{algorithm.12}}
\newlabel{eq:jointcfg}{{4.1}{62}{Probabilistic Context-Free Grammars\relax }{equation.4.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}The CKY Parsing Algorithm}{62}{subsection.4.1.4}}
\newlabel{exer:cky}{{4.1}{62}{The CKY Parsing Algorithm\relax }{exercise.4.1}{}}
\citation{Manning1999}
\citation{Klein2002}
\citation{Smith2005}
\citation{Cohen2008}
\citation{Johnson1998}
\citation{Klein2003}
\citation{Magerman1995}
\citation{Charniak1997}
\citation{Collins1999}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.5}Learning the Grammar}{63}{subsection.4.1.5}}
\newlabel{exer:treebank}{{4.2}{63}{Learning the Grammar\relax }{exercise.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.6}Model Refinements}{63}{subsection.4.1.6}}
\citation{Taskar2004}
\citation{Finkel2008}
\citation{Petrov2007NAACL}
\citation{Petrov2008NIPS}
\citation{Petrov2008EMNLP}
\citation{Charniak2006}
\citation{Ratnaparkhi1999}
\citation{Henderson2003}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A lexicalized parse tree for the sentence \emph  {She enjoys the Summer school}.}}{64}{figure.4.1}}
\newlabel{fig:lexicalizedtree}{{4.1}{64}{A lexicalized parse tree for the sentence \emph {She enjoys the Summer school}}{figure.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Dependency Parsing}{64}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Motivation}{64}{subsection.4.2.1}}
\citation{Chomsky1965}
\citation{Tesniere1959}
\citation{Hudson1984}
\citation{Melcuk1988}
\citation{Covington1990}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces A dependency tree for the sentence \emph  {She enjoys the Summer school}. Note the additional dummy root symbol (*) which is included for convenience.}}{65}{figure.4.2}}
\newlabel{fig:deptree_proj}{{4.2}{65}{A dependency tree for the sentence \emph {She enjoys the Summer school}. Note the additional dummy root symbol (*) which is included for convenience}{figure.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Projective and Non-projective Parsing}{65}{subsection.4.2.2}}
\citation{Eisner1996}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces A non-projective parse tree.}}{66}{figure.4.3}}
\newlabel{fig:deptree_nonproj}{{4.3}{66}{A non-projective parse tree}{figure.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Algorithms for Projective Dependency Parsing}{66}{subsection.4.2.3}}
\citation{Chu1965}
\citation{Edmonds1967}
\citation{Tarjan1977}
\citation{McDonald2005b}
\citation{Tutte1984}
\citation{DSmithSmith2007}
\citation{Koo2007}
\citation{McDonald2007}
\citation{conll06st}
\citation{Surdeanu2008}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Algorithms for Non-Projective Dependency Parsing}{67}{subsection.4.2.4}}
\citation{Eisner1999}
\citation{Carreras2007}
\citation{McDonald2007}
\citation{McDonald2006CoNLL}
\citation{DSmith2008}
\citation{Martins2009ACL}
\citation{Koo2010EMNLP}
\citation{Koo2010}
\citation{Nivre2006CoNLL}
\citation{Huang2010}
\citation{Nivre2009}
\bibstyle{apalike}
\bibdata{guide}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Model Refinements}{69}{subsection.4.2.5}}
