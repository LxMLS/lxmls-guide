\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {0}Basic Tutorials}{1}{chapter.0}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Python}{1}{section.0.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.1}Python Basics}{1}{subsection.0.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{Running Python code}{1}{section*.1}}
\citation{PER-GRA:2007}
\@writefile{toc}{\contentsline {subsubsection}{Help and Documentation}{2}{section*.2}}
\@writefile{toc}{\contentsline {subsubsection}{Exiting}{2}{section*.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.2}Python by Example}{2}{subsection.0.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{Data Structures}{2}{section*.4}}
\@writefile{toc}{\contentsline {subsubsection}{Loops and Indentation}{3}{section*.5}}
\@writefile{toc}{\contentsline {subsubsection}{Control Flow}{3}{section*.6}}
\@writefile{toc}{\contentsline {subsubsection}{Functions}{4}{section*.7}}
\@writefile{toc}{\contentsline {subsubsection}{Profiling}{4}{section*.8}}
\@writefile{toc}{\contentsline {subsubsection}{Debugging in Python}{5}{section*.9}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Basic pdb/ipdb commands, parentheses indicates abbreviation}}{5}{table.0.1}}
\newlabel{tb::pdbbasiccommands}{{1}{5}{Basic pdb/ipdb commands, parentheses indicates abbreviation}{table.0.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.3}Exceptions}{6}{subsection.0.1.3}}
\@writefile{toc}{\contentsline {subsubsection}{Extending basic Functionalities with Modules}{6}{section*.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.4}Matplotlib -- Plotting in Python}{7}{subsection.0.1.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.5}Numpy -- Scientific Computing with Python}{7}{subsection.0.1.5}}
\@writefile{toc}{\contentsline {subsubsection}{Multidimensional Arrays}{7}{section*.11}}
\@writefile{toc}{\contentsline {subsubsection}{Mathematical Operations}{8}{section*.12}}
\@writefile{toc}{\contentsline {section}{\numberline {0.2}Essential Linear Algebra}{9}{section.0.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.1}Notation}{9}{subsection.0.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.2}Some Matrix Operations and Properties}{10}{subsection.0.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.3}Norms}{11}{subsection.0.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.4}Linear Independence, Rank, and Orthogonal Matrices}{12}{subsection.0.2.4}}
\@writefile{toc}{\contentsline {section}{\numberline {0.3}Probability Theory}{12}{section.0.3}}
\newlabel{eq:condprob}{{2}{13}{Probability Theory}{equation.0.3.2}{}}
\newlabel{eq:totprob}{{3}{13}{Probability Theory}{equation.0.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.1}Probability distribution functions}{13}{subsection.0.3.1}}
\newlabel{bernoulli}{{0.3.2}{14}{Bernoulli}{subsection.0.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.2} Bernoulli}{14}{subsection.0.3.2}}
\newlabel{bernoulli-eq}{{0.3.2}{14}{Bernoulli}{subsection.0.3.2}{}}
\newlabel{binomial}{{0.3.3}{14}{Binomial}{subsection.0.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.3} Binomial}{14}{subsection.0.3.3}}
\newlabel{categorical}{{0.3.4}{14}{Categorical}{subsection.0.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.4} Categorical}{14}{subsection.0.3.4}}
\newlabel{multinomial}{{0.3.5}{14}{Multinomial}{subsection.0.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.5} Multinomial}{14}{subsection.0.3.5}}
\newlabel{gaussian}{{0.3.6}{15}{Gaussian Distribution}{subsection.0.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.6} Gaussian Distribution}{15}{subsection.0.3.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Normal density for two sets of parameter values.}}{15}{figure.0.1}}
\newlabel{fig:normaldist}{{1}{15}{Normal density for two sets of parameter values}{figure.0.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.7}Maximum Likelihood Estimation}{15}{subsection.0.3.7}}
\citation{Nocedal1999}
\citation{bertsekas1995np}
\citation{boyd2004convex}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.8}Conjugate Priors}{17}{subsection.0.3.8}}
\newlabel{numerical_optimization}{{0.4}{17}{Numerical optimization}{section.0.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {0.4}Numerical optimization}{17}{section.0.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.4.1}Convex Functions}{17}{subsection.0.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.4.2}Derivative and Gradient}{17}{subsection.0.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Illustration of a convex function. The line segment between any two points on the graph lies entirely above the curve.}}{18}{figure.0.2}}
\newlabel{fig:convexfn}{{2}{18}{Illustration of a convex function. The line segment between any two points on the graph lies entirely above the curve}{figure.0.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Illustration of a non-convex function. Note the line segment intersecting the curve. }}{18}{figure.0.3}}
\newlabel{fig:nonconvexfn}{{3}{18}{Illustration of a non-convex function. Note the line segment intersecting the curve}{figure.0.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Some derivative examples}}{18}{table.0.2}}
\newlabel{tb::derivatives}{{2}{18}{Some derivative examples}{table.0.2}{}}
\citation{Nocedal1999}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Illustration of the gradient of the function $f(x^2)$ at three different points $x = [-2,0.2]$. Note that at point $x = 0$ the gradient is zero which corresponds to the minimum of the function.}}{19}{figure.0.4}}
\newlabel{fig:tangents}{{4}{19}{Illustration of the gradient of the function $f(x^2)$ at three different points $x = [-2,0.2]$. Note that at point $x = 0$ the gradient is zero which corresponds to the minimum of the function}{figure.0.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.4.3}Gradient Based Methods}{19}{subsection.0.4.3}}
\citation{Nocedal1999}
\newlabel{alg:graddescent}{{1}{20}{Gradient Based Methods}{algorithm.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Gradient Descent}}{20}{algorithm.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Illustration of gradient descent. The blue circles correspond to contours of the function (each blue circle is a set of points which have the same function value), while the red lines correspond to steps taken in the negative gradient direction.}}{21}{figure.0.5}}
\newlabel{fig:graddescent}{{5}{21}{Illustration of gradient descent. The blue circles correspond to contours of the function (each blue circle is a set of points which have the same function value), while the red lines correspond to steps taken in the negative gradient direction}{figure.0.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Example of running gradient descent starting on point $x_0 = -8$ for function $f(x) = (x+2)^2 - 16 \qopname  \relax o{exp}\left ( -(x-2)^2 \right )$. The function is represented in blue, while the points of the minimizing sequence are displayed as green plus signs.}}{22}{figure.0.6}}
\newlabel{fig:gradex1}{{6}{22}{Example of running gradient descent starting on point $x_0 = -8$ for function $f(x) = (x+2)^2 - 16 \exp \left ( -(x-2)^2 \right )$. The function is represented in blue, while the points of the minimizing sequence are displayed as green plus signs}{figure.0.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {0.5}Python Exercises}{22}{section.0.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.5.1}Numpy and Matplotlib}{22}{subsection.0.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Example of running gradient descent starting on point $x_0 = 8$ for function $f(x) = (x+2)^2 - 16 \qopname  \relax o{exp}\left ( -(x-2)^2 \right )$. The function is represented in blue, while the points of the minimizing sequence are displayed as green plus signs.}}{23}{figure.0.7}}
\newlabel{fig:gradex2}{{7}{23}{Example of running gradient descent starting on point $x_0 = 8$ for function $f(x) = (x+2)^2 - 16 \exp \left ( -(x-2)^2 \right )$. The function is represented in blue, while the points of the minimizing sequence are displayed as green plus signs}{figure.0.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.5.2}Debugging}{24}{subsection.0.5.2}}
\citation{Mitchell1997}
\citation{Duda2001}
\citation{Schoelkopf2002}
\citation{Joachims2002}
\citation{Bishop2006}
\citation{Manning2008}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Classification}{26}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{day:classification}{{1}{26}{Classification}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Pre-assignment}{26}{section.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Notation}{26}{subsection.1.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces  Example of a dataset. The input set consists in points in the real plane, $\EuScript  {X} = \PazoBB  {R}^2$, and the output set consists of two classes (Red and Blue). Training points are represented as squares, while test points are represented as circles.}}{27}{figure.1.1}}
\newlabel{simpleDataSet}{{1.1}{27}{Example of a dataset. The input set consists in points in the real plane, $\mathcal {X} = \mathbb {R}^2$, and the output set consists of two classes (Red and Blue). Training points are represented as squares, while test points are represented as circles}{figure.1.1}{}}
\newlabel{s::naiveBayes}{{1.1.2}{27}{Generative Classifiers: Na\"{i}ve Bayes}{subsection.1.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Generative Classifiers: Na\"{i}ve Bayes}{27}{subsection.1.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces  Example of a dataset together with the corresponding Bayes optimal decision boundary. The input set consists in points in the real plane, $\EuScript  {X} = \EuScript  {R^2}$, and the output set consists of two classes (Red and Blue). Training points are represented as squares, while test points are represented as circles.}}{28}{figure.1.2}}
\newlabel{simpleDataSet_bo}{{1.2}{28}{Example of a dataset together with the corresponding Bayes optimal decision boundary. The input set consists in points in the real plane, $\mathcal {X} = \mathcal {R^2}$, and the output set consists of two classes (Red and Blue). Training points are represented as squares, while test points are represented as circles}{figure.1.2}{}}
\newlabel{eq:argmax}{{1.4}{28}{Generative Classifiers: Na\"{i}ve Bayes}{equation.1.1.4}{}}
\citation{blitzer2007biographies}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Example: Multinomial Na\"{i}ve Bayes for Document Classification}{29}{subsection.1.1.3}}
\newlabel{eq:mlemultinomial}{{1.9}{29}{Example: Multinomial Na\"{i}ve Bayes for Document Classification}{equation.1.1.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Assignment}{29}{section.1.2}}
\citation{Manning1999}
\citation{Manning2008}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Post-assignment}{30}{section.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Features and Discriminative Classifiers}{30}{subsection.1.3.1}}
\newlabel{sec:linearclass}{{1.3.1}{30}{Features and Discriminative Classifiers}{subsection.1.3.1}{}}
\newlabel{eq:jointfeatsimple}{{1.16}{31}{Features and Discriminative Classifiers}{equation.1.3.16}{}}
\citation{Rosenblatt1958}
\newlabel{alg:perceptron}{{2}{32}{Perceptron}{algorithm.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces  Averaged perceptron}}{32}{algorithm.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Online Discriminative Algorithms: Perceptron and MIRA}{32}{subsection.1.3.2}}
\newlabel{s:perceptron}{{1.3.2}{32}{Perceptron}{section*.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{ Perceptron}{32}{section*.14}}
\citation{Crammer2002}
\citation{Crammer2006}
\@writefile{toc}{\contentsline {subsubsection}{Margin Infused Relaxed Algorithm (MIRA)}{33}{section*.15}}
\newlabel{eq:miraupdates}{{1.22}{33}{Margin Infused Relaxed Algorithm (MIRA)}{equation.1.3.22}{}}
\citation{Crammer2006}
\citation{Shannon1948}
\citation{Jaynes1982}
\citation{Cover1991}
\newlabel{alg:mira}{{3}{34}{Margin Infused Relaxed Algorithm (MIRA)}{algorithm.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces MIRA }}{34}{algorithm.3}}
\newlabel{eq:costfunc}{{1.25}{34}{Margin Infused Relaxed Algorithm (MIRA)}{equation.1.3.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Batch Discriminative Classifiers: Maximum Entropy and Support Vector Machines}{34}{subsection.1.3.3}}
\newlabel{s:me}{{1.3.3}{35}{Maximum Entropy Classifiers}{section*.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{Maximum Entropy Classifiers}{35}{section*.16}}
\newlabel{eq:loglinear}{{1.26}{35}{Maximum Entropy Classifiers}{equation.1.3.26}{}}
\citation{Nocedal1999}
\newlabel{eq:maxent}{{1.30}{36}{Maximum Entropy Classifiers}{equation.1.3.30}{}}
\citation{Vapnik1995}
\newlabel{alg:maxent_gd}{{4}{37}{Maximum Entropy Classifiers}{algorithm.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Batch Gradient Descent for Maximum Entropy }}{37}{algorithm.4}}
\@writefile{toc}{\contentsline {subsubsection}{Support Vector Machines}{37}{section*.17}}
\newlabel{sec:svms}{{1.3.3}{37}{Support Vector Machines}{section*.17}{}}
\citation{Crammer2002}
\citation{Schoelkopf2002}
\citation{ShaweTaylor2004}
\newlabel{alg:maxent_sgd}{{5}{38}{Maximum Entropy Classifiers}{algorithm.5}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces SGD for Maximum Entropy }}{38}{algorithm.5}}
\newlabel{eq:hingeloss}{{1.33}{38}{Support Vector Machines}{equation.1.3.33}{}}
\newlabel{eq:svm}{{1.34}{38}{Support Vector Machines}{equation.1.3.34}{}}
\newlabel{eq:svm2}{{1.35}{38}{Support Vector Machines}{equation.1.3.35}{}}
\citation{ShalevShwartz2007ICML}
\newlabel{alg:svm_ssd}{{6}{39}{Support Vector Machines}{algorithm.6}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Stochastic Subgradient Descent for SVMs }}{39}{algorithm.6}}
\newlabel{tab:comparison_lab1}{{1.3.4}{40}{Comparison}{subsection.1.3.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1.1}{\ignorespaces Comparison among different models.}}{40}{table.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.4}Comparison}{40}{subsection.1.3.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.5}Final remarks}{40}{subsection.1.3.5}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Sequence Models}{41}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{day:seq}{{2}{41}{Sequence Models}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Notation}{41}{section.2.1}}
\newlabel{hmm}{{2.2}{41}{Hidden Markov Models}{section.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2} Hidden Markov Models}{41}{section.2.2}}
\newlabel{tab:hmm_notation}{{2.1}{42}{Notation}{Item.30}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces General notation used in this class}}{42}{table.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces HMM running example}}{43}{figure.2.1}}
\newlabel{fig:hmm}{{2.1}{43}{HMM running example}{figure.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces HMM notation}}{43}{table.2.2}}
\newlabel{tab:hmm-simple-notation}{{2.2}{43}{HMM notation}{table.2.2}{}}
\newlabel{eqn:hmm}{{2.1}{43}{Hidden Markov Models}{equation.2.2.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces HMM probability distributions}}{44}{table.2.3}}
\newlabel{tab:hmm-dist}{{2.3}{44}{HMM probability distributions}{table.2.3}{}}
\newlabel{eqn:hmm_ex}{{2.2}{44}{Hidden Markov Models}{equation.2.2.2}{}}
\newlabel{ml}{{2.3}{44}{Finding the Maximum Likelihood Parameters}{section.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3} Finding the Maximum Likelihood Parameters}{44}{section.2.3}}
\newlabel{eq::initialCounts}{{2.4}{44}{Finding the Maximum Likelihood Parameters}{equation.2.3.4}{}}
\newlabel{eq::transitionCounts}{{2.5}{44}{Finding the Maximum Likelihood Parameters}{equation.2.3.5}{}}
\newlabel{eq::finalCounts}{{2.6}{44}{Finding the Maximum Likelihood Parameters}{equation.2.3.6}{}}
\newlabel{eq::emissionCounts}{{2.7}{44}{Finding the Maximum Likelihood Parameters}{equation.2.3.7}{}}
\newlabel{decoding}{{2.4}{45}{Decoding a Sequence}{section.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4} Decoding a Sequence}{45}{section.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces HMM Trellis representation.}}{46}{figure.2.2}}
\newlabel{fig:trellis}{{2.2}{46}{HMM Trellis representation}{figure.2.2}{}}
\citation{rabiner}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Computing in log-domain}{47}{subsection.2.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Posterior Decoding}{48}{subsection.2.4.2}}
\newlabel{eq::posteriorDistribution}{{2.21}{48}{Posterior Decoding}{equation.2.4.21}{}}
\newlabel{eq::nodePosterior}{{2.22}{48}{Posterior Decoding}{equation.2.4.22}{}}
\newlabel{eq::edgePosterior}{{2.23}{48}{Posterior Decoding}{equation.2.4.23}{}}
\newlabel{likelihoood}{{2.24}{48}{Posterior Decoding}{equation.2.4.24}{}}
\newlabel{eq::forward}{{2.25}{48}{Posterior Decoding}{equation.2.4.25}{}}
\newlabel{forwardRecursion}{{2.27}{48}{Posterior Decoding}{equation.2.4.26}{}}
\newlabel{eq:forwardSum}{{2.29}{48}{Posterior Decoding}{equation.2.4.29}{}}
\newlabel{eq::backward}{{2.30}{48}{Posterior Decoding}{equation.2.4.30}{}}
\newlabel{alg:fb}{{7}{49}{Posterior Decoding}{algorithm.7}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7}{\ignorespaces Forward-Backward algorithm }}{49}{algorithm.7}}
\newlabel{backwardRecursion}{{2.32}{49}{Posterior Decoding}{equation.2.4.31}{}}
\newlabel{eq:backwardSum}{{2.34}{49}{Posterior Decoding}{equation.2.4.34}{}}
\newlabel{eq::fbsanity}{{2.35}{49}{Posterior Decoding}{equation.2.4.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Posterior Illustration.}}{50}{figure.2.3}}
\newlabel{fig:posteriors}{{2.3}{50}{Posterior Illustration}{figure.2.3}{}}
\newlabel{eq::nodePosterior2}{{2.36}{50}{Posterior Decoding}{equation.2.4.36}{}}
\newlabel{eq::edgePosterior2}{{2.37}{50}{Posterior Decoding}{equation.2.4.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.3}Viterbi Decoding}{51}{subsection.2.4.3}}
\newlabel{eq::viterbi}{{2.40}{51}{Viterbi Decoding}{equation.2.4.40}{}}
\newlabel{alg:viterbi}{{8}{52}{Viterbi Decoding}{algorithm.8}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {8}{\ignorespaces Viterbi algorithm }}{52}{algorithm.8}}
\newlabel{viterbiRecursion}{{2.42}{52}{Viterbi Decoding}{equation.2.4.41}{}}
\citation{pennTreeBank}
\newlabel{pos-tagging}{{2.5}{53}{Part-of-Speech Tagging (POS)}{section.2.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.5} Part-of-Speech Tagging (POS)}{53}{section.2.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces  Transition probabilities of the trained model. Each column is previous state and row is current state. Note the high probability of having Noun after Adjective, or of having Verb after Noun, as expected.}}{54}{figure.2.4}}
\newlabel{fig:transProbs}{{2.4}{54}{Transition probabilities of the trained model. Each column is previous state and row is current state. Note the high probability of having Noun after Adjective, or of having Verb after Noun, as expected}{figure.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces  Confusion Matrix for the previous example. Predict tags are columns and the true tags corresponds to the constituents of each column.}}{55}{figure.2.5}}
\newlabel{fig:cm_uns}{{2.5}{55}{Confusion Matrix for the previous example. Predict tags are columns and the true tags corresponds to the constituents of each column}{figure.2.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Unsupervised Learning of HMMs}{55}{section.2.6}}
\citation{collins2002discriminative}
\citation{lafferty2001conditional}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Learning Structured Predictors}{57}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{day:seq_disc}{{3}{57}{Learning Structured Predictors}{chapter.3}{}}
\newlabel{eq::struc_pred}{{3.1}{57}{Learning Structured Predictors}{equation.3.0.1}{}}
\newlabel{eq::struc_pred_decompose}{{3.2}{57}{Learning Structured Predictors}{equation.3.0.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Summary of the methods that we will be covering this lecture.}}{57}{table.3.1}}
\newlabel{disc_seq_summary}{{3.1}{57}{Summary of the methods that we will be covering this lecture}{table.3.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces  {\tt  IDFeatures} feature set. This set replicates the features used by the HMM model.}}{58}{table.3.2}}
\newlabel{id-features}{{3.2}{58}{{\tt IDFeatures} feature set. This set replicates the features used by the HMM model}{table.3.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces  Extended feature set. Some features in this set could not be included in the HMM model. The features included in the bottom row are all considered emission features for the purpose of our implementation, since they all depend on $i$, $x$ and $y_i$.}}{58}{table.3.3}}
\newlabel{ex-features}{{3.3}{58}{Extended feature set. Some features in this set could not be included in the HMM model. The features included in the bottom row are all considered emission features for the purpose of our implementation, since they all depend on $i$, $x$ and $y_i$}{table.3.3}{}}
\newlabel{seq::features}{{3.1}{58}{Feature Extraction}{section.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1} Feature Extraction}{58}{section.3.1}}
\citation{lafferty2001conditional}
\citation{mccallum2000maximum}
\citation{Bottou1991}
\citation{lafferty2001conditional}
\newlabel{dis_node_potentials}{{3.6}{59}{Feature Extraction}{equation.3.1.6}{}}
\newlabel{eq:disc_formula}{{3.7}{59}{Feature Extraction}{equation.3.1.7}{}}
\newlabel{alg:crf_online}{{9}{60}{Conditional Random Fields}{algorithm.9}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {9}{\ignorespaces SGD for Conditional Random Fields }}{60}{algorithm.9}}
\newlabel{s:crf}{{3.2}{60}{Conditional Random Fields}{section.3.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Conditional Random Fields}{60}{section.3.2}}
\newlabel{exer:crf1}{{3.1}{60}{Conditional Random Fields}{exercise.3.1}{}}
\citation{collins2002discriminative}
\newlabel{exer:crf2}{{3.2}{62}{Conditional Random Fields}{exercise.3.2}{}}
\newlabel{alg:structured-perceptron}{{10}{63}{Structured Perceptron}{algorithm.10}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {10}{\ignorespaces Averaged Structured perceptron }}{63}{algorithm.10}}
\newlabel{s:spercetron}{{3.3}{63}{Structured Perceptron}{section.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Structured Perceptron}{63}{section.3.3}}
\newlabel{exer:strucperc1}{{3.3}{63}{Structured Perceptron}{exercise.3.3}{}}
\citation{Hopcroft1979}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Syntax and Parsing}{66}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Phrase-based Parsing}{66}{section.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Context Free Grammars}{66}{subsection.4.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Ambiguity}{67}{subsection.4.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.3}Probabilistic Context-Free Grammars}{68}{subsection.4.1.3}}
\citation{Manning1999}
\newlabel{alg:cky}{{11}{69}{The CKY Parsing Algorithm}{algorithm.11}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {11}{\ignorespaces CKY algorithm }}{69}{algorithm.11}}
\newlabel{eq:jointcfg}{{4.1}{69}{Probabilistic Context-Free Grammars}{equation.4.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.4}The CKY Parsing Algorithm}{69}{subsection.4.1.4}}
\newlabel{exer:cky}{{4.1}{69}{The CKY Parsing Algorithm}{exercise.4.1}{}}
\citation{Manning1999}
\citation{Klein2002}
\citation{Smith2005}
\citation{Cohen2008}
\citation{Johnson1998}
\citation{Klein2003}
\citation{Magerman1995}
\citation{Charniak1997}
\citation{Collins1999}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.5}Learning the Grammar}{70}{subsection.4.1.5}}
\newlabel{exer:treebank}{{4.2}{70}{Learning the Grammar}{exercise.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.6}Model Refinements}{70}{subsection.4.1.6}}
\citation{Taskar2004}
\citation{Finkel2008}
\citation{Petrov2007NAACL}
\citation{Petrov2008NIPS}
\citation{Petrov2008EMNLP}
\citation{Charniak2006}
\citation{Ratnaparkhi1999}
\citation{Henderson2003}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces A lexicalized parse tree for the sentence \emph  {She enjoys the Summer school}.}}{71}{figure.4.1}}
\newlabel{fig:lexicalizedtree}{{4.1}{71}{A lexicalized parse tree for the sentence \emph {She enjoys the Summer school}}{figure.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Dependency Parsing}{71}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Motivation}{71}{subsection.4.2.1}}
\citation{Chomsky1965}
\citation{Tesniere1959}
\citation{Hudson1984}
\citation{Melcuk1988}
\citation{Covington1990}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces A dependency tree for the sentence \emph  {She enjoys the Summer school}. Note the additional dummy root symbol (*) which is included for convenience.}}{72}{figure.4.2}}
\newlabel{fig:deptree_proj}{{4.2}{72}{A dependency tree for the sentence \emph {She enjoys the Summer school}. Note the additional dummy root symbol (*) which is included for convenience}{figure.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Projective and Non-projective Parsing}{72}{subsection.4.2.2}}
\citation{Eisner1996}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces A non-projective parse tree.}}{73}{figure.4.3}}
\newlabel{fig:deptree_nonproj}{{4.3}{73}{A non-projective parse tree}{figure.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Algorithms for Projective Dependency Parsing}{73}{subsection.4.2.3}}
\citation{Chu1965}
\citation{Edmonds1967}
\citation{Tarjan1977}
\citation{McDonald2005b}
\citation{Tutte1984}
\citation{DSmithSmith2007}
\citation{Koo2007}
\citation{McDonald2007}
\citation{conll06st}
\citation{Surdeanu2008}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Algorithms for Non-Projective Dependency Parsing}{74}{subsection.4.2.4}}
\citation{Eisner1999}
\citation{Carreras2007}
\citation{McDonald2007}
\citation{McDonald2006CoNLL}
\citation{DSmith2008}
\citation{Martins2009ACL}
\citation{Koo2010EMNLP}
\citation{Koo2010}
\citation{Nivre2006CoNLL}
\citation{Huang2010}
\citation{Nivre2009}
\bibstyle{apalike}
\bibdata{guide}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.5}Model Refinements}{76}{subsection.4.2.5}}
\bibcite{bertsekas1995np}{{1}{1995}{{Bertsekas et~al.}}{{}}}
\bibcite{Bishop2006}{{2}{2006}{{Bishop}}{{}}}
\bibcite{blitzer2007biographies}{{3}{2007}{{Blitzer et~al.}}{{}}}
\bibcite{Bottou1991}{{4}{1991}{{Bottou}}{{}}}
\bibcite{boyd2004convex}{{5}{2004}{{Boyd and Vandenberghe}}{{}}}
\bibcite{conll06st}{{6}{2006}{{Buchholz and Marsi}}{{}}}
\bibcite{Carreras2007}{{7}{2007}{{Carreras}}{{}}}
\bibcite{Charniak1997}{{8}{1997}{{Charniak}}{{}}}
\bibcite{Charniak2006}{{9}{2006}{{Charniak et~al.}}{{}}}
\bibcite{Chomsky1965}{{10}{1965}{{Chomsky}}{{}}}
\bibcite{Chu1965}{{11}{1965}{{Chu and Liu}}{{}}}
\bibcite{Cohen2008}{{12}{2008}{{Cohen et~al.}}{{}}}
\bibcite{Collins1999}{{13}{1999}{{Collins}}{{}}}
\bibcite{collins2002discriminative}{{14}{2002}{{Collins}}{{}}}
\bibcite{Cover1991}{{15}{1991}{{Cover et~al.}}{{}}}
\bibcite{Covington1990}{{16}{1990}{{Covington}}{{}}}
\bibcite{Crammer2006}{{17}{2006}{{Crammer et~al.}}{{}}}
\bibcite{Crammer2002}{{18}{2002}{{Crammer and Singer}}{{}}}
\bibcite{Duda2001}{{19}{2001}{{Duda et~al.}}{{}}}
\bibcite{Edmonds1967}{{20}{1967}{{Edmonds}}{{}}}
\bibcite{Eisner1996}{{21}{1996}{{Eisner}}{{}}}
\bibcite{Eisner1999}{{22}{1999}{{Eisner and Satta}}{{}}}
\bibcite{Finkel2008}{{23}{2008}{{Finkel et~al.}}{{}}}
\bibcite{Henderson2003}{{24}{2003}{{Henderson}}{{}}}
\bibcite{Hopcroft1979}{{25}{1979}{{Hopcroft et~al.}}{{}}}
\bibcite{Huang2010}{{26}{2010}{{Huang and Sagae}}{{}}}
\bibcite{Hudson1984}{{27}{1984}{{Hudson}}{{}}}
\bibcite{Jaynes1982}{{28}{1982}{{Jaynes}}{{}}}
\bibcite{Joachims2002}{{29}{2002}{{Joachims}}{{}}}
\bibcite{Johnson1998}{{30}{1998}{{Johnson}}{{}}}
\bibcite{Klein2002}{{31}{2002}{{Klein and Manning}}{{}}}
\bibcite{Klein2003}{{32}{2003}{{Klein and Manning}}{{}}}
\bibcite{Koo2010}{{33}{2010}{{Koo and Collins}}{{}}}
\bibcite{Koo2007}{{34}{2007}{{Koo et~al.}}{{}}}
\bibcite{Koo2010EMNLP}{{35}{2010}{{Koo et~al.}}{{}}}
\bibcite{lafferty2001conditional}{{36}{2001}{{Lafferty et~al.}}{{}}}
\bibcite{Magerman1995}{{37}{1995}{{Magerman}}{{}}}
\bibcite{Manning2008}{{38}{2008}{{Manning et~al.}}{{}}}
\bibcite{Manning1999}{{39}{1999}{{Manning and Sch{\"u}tze}}{{}}}
\bibcite{pennTreeBank}{{40}{1993}{{Marcus et~al.}}{{}}}
\bibcite{Martins2009ACL}{{41}{2009}{{Martins et~al.}}{{}}}
\bibcite{mccallum2000maximum}{{42}{2000}{{McCallum et~al.}}{{}}}
\bibcite{McDonald2006CoNLL}{{43}{2006}{{McDonald et~al.}}{{}}}
\bibcite{McDonald2007}{{44}{2007}{{McDonald and Satta}}{{}}}
\bibcite{McDonald2005b}{{45}{2005}{{McDonald et~al.}}{{}}}
\bibcite{Melcuk1988}{{46}{1988}{{Melʹ{\v {c}}uk}}{{}}}
\bibcite{Mitchell1997}{{47}{1997}{{Mitchell}}{{}}}
\bibcite{Nivre2009}{{48}{2009}{{Nivre}}{{}}}
\bibcite{Nivre2006CoNLL}{{49}{2006}{{Nivre et~al.}}{{}}}
\bibcite{Nocedal1999}{{50}{1999}{{Nocedal and Wright}}{{}}}
\bibcite{PER-GRA:2007}{{51}{2007}{{P\'erez and Granger}}{{}}}
\bibcite{Petrov2007NAACL}{{52}{2007}{{Petrov and Klein}}{{}}}
\bibcite{Petrov2008NIPS}{{53}{2008a}{{Petrov and Klein}}{{}}}
\bibcite{Petrov2008EMNLP}{{54}{2008b}{{Petrov and Klein}}{{}}}
\bibcite{rabiner}{{55}{1989}{{Rabiner}}{{}}}
\bibcite{Ratnaparkhi1999}{{56}{1999}{{Ratnaparkhi}}{{}}}
\bibcite{Rosenblatt1958}{{57}{1958}{{Rosenblatt}}{{}}}
\bibcite{Schoelkopf2002}{{58}{2002}{{Sch{\"o}lkopf and Smola}}{{}}}
\bibcite{ShalevShwartz2007ICML}{{59}{2007}{{Shalev-Shwartz et~al.}}{{}}}
\bibcite{Shannon1948}{{60}{1948}{{Shannon}}{{}}}
\bibcite{ShaweTaylor2004}{{61}{2004}{{Shawe-Taylor and Cristianini}}{{}}}
\bibcite{DSmith2008}{{62}{2008}{{Smith and Eisner}}{{}}}
\bibcite{DSmithSmith2007}{{63}{2007}{{Smith and Smith}}{{}}}
\bibcite{Smith2005}{{64}{2005}{{Smith and Eisner}}{{}}}
\bibcite{Surdeanu2008}{{65}{2008}{{Surdeanu et~al.}}{{}}}
\bibcite{Tarjan1977}{{66}{1977}{{Tarjan}}{{}}}
\bibcite{Taskar2004}{{67}{2004}{{Taskar et~al.}}{{}}}
\bibcite{Tesniere1959}{{68}{1959}{{Tesni{\`e}re}}{{}}}
\bibcite{Tutte1984}{{69}{1984}{{Tutte}}{{}}}
\bibcite{Vapnik1995}{{70}{1995}{{Vapnik}}{{}}}
