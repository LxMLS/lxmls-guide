\section{Today's assignment}
Today's class will be divided in two main parts. First, we will learn the main 
algorithmic principle behind deep learning and code the Backpropagation algorithm 
in Numpy. Second, we will learn the basics of the Theano module for Python, 
which allows to easily implement deep learning techniques that run on a 
Graphical Processing Unit (GPU) for optimal speed. 

If you are new to the topic you should aim to finish the deep learning 
principles part (Ex. 6.1 and 6.2) and at least Ex 6.3 and 6.4 of the Theano 
part. If you already know Backpropagation well and have experience with normal 
Python, you should aim to complete the whole day. 

\section{Introduction to Deep Learning and Theano}

Deep learning is the name behind the latest wave of successful neural network 
research, a very old topic dating from the first half of the 20th century. Deep 
learning techniques have attained formidable impact in the machine learning 
community. 
Some of the changes that led to this renewed success are, not only
improvements and insight on the existing neural network algorithms, but also
the increase in the amount of data available and computing power. In
particular, the use of Graphical Processing Units (GPUs) has allowed neural
networks to be applied to very large datasets. Working with GPUs is not trivial
as it requires dealing with specialized hardware. Luckily, as it is often the
case, we are one Python import away from solving this problem. For the
particular case of deep learning, the
Theano\footnotemark\footnotetext{http://deeplearning.net/software/theano/}
module allows us to express models symbolically, automatically computing
gradients and solve overflow problems. Furthermore, the code is also ready to
use with CUDA-compatible GPUs.

There is nothing particularly difficult in the deep learning concept. You have
already visited all the mathematical principles you need in the first days of the
labs of this school. At their core, deep learning models are just functions
mapping vector inputs $\mathbf{x}$ to vector outputs $\mathbf{y}$, just like any
of the models we saw on previous days. Deep learning models are constructed by
composing linear and non-linear transformations to build structures that vaguely
resemble human neural networks, hence the name artificial neural networks. Due
to their compositional nature, gradient methods and the chain rule are used to
learn the parameters of these models. See Section \ref{unsupervised} for a
refresh on the basic concept. We will also refer to the gradient
learning methods introduced in Section \ref{s:me}. 

\section{Non-Linear functions as computation graphs} 

\subsection{Example: The computation graph of a log-linear model}

%The most basic architecture used in deep learning is the feed forward network,
%also known as Multi-Layer Perceptron (MLP). This is a non-linear model built by
%alternatively composing linear and non-linear transformations. The log-linear
%models we saw on Day 1 can be interpreted as a special case of MLPs with just
%one layer and a soft-max non-linearity. For this reason, we will use them as an
%initial example to introduce the concept, and progress from there on.

Recall the maximum entropy classifier in Section \ref{s:me}. This was a log-linear
model that, given an input vector of features $\boldsymbol{f}(x,y)$, would
predict the probability of that vector belonging to each of $K$ possible
classes $y_k$. For the purpose of introducing deep learning, we will use a more
general form given by 
%
\begin{align}
p(y_k|\mathbf{x}) & = \frac{1}{Z}\exp(\mathbf{W}_k \cdot \mathbf{x} + b_k)
\label{eq:loglineargen}
\end{align}
%
where the partition function $Z$ is given by
%
\begin{align}
Z(\mathbf{W},\mathbf{b},\mathbf{x}) = \sum_{k'=0}^{K-1} \exp(\mathbf{W}_{k'} \cdot \mathbf{x} + b_{k'}). 
\label{eq:loglineargenPartition}
\end{align}
%
There are small differences with respect to  Eq:\ref{eq:loglinear}. The model
parameters $\Theta=\{\mathbf{W}, \mathbf{b}\}$ include not only weights
$\mathbf{W} \in \mathbb{R}^{K \times I}$ but also a bias $\mathbf{b} \in
\mathbb{R}^{K \times 1}$. Also $\mathbf{x} \in \mathbb{R}^{I \times 1}$ is a
generic input vector of real valued features of dimension $I$.  The binary
features $\boldsymbol{f}(x,y)$ used in the maximum entropy classifier example
can be considered a special case of $\mathbf{x}$.    

\noindent One useful way of seeing \ref{eq:loglineargen} is as a composition of
linear and non-linear transformations as the one displayed in \ref{fig:LayerP} 

\begin{equation}
p(y_k|\mathbf{x}) \equiv \tilde{z}_k = (f_k \circ \mathbf{g})(\mathbf{x}),
\end{equation}

\noindent where $\circ$ indicates function composition. To obtain $\tilde{z}_k$ the
linear transformation 

\begin{equation}
\mathbf{z} = \mathbf{g}(\mathbf{x}) = \mathbf{W} \cdot \mathbf{x} + \mathbf{b}.
\label{eq:linear}
\end{equation}

\noindent yields an intermediate vector variable $\mathbf{z} \in \mathbb{R}^{K \times
1}$, which is then transformed through the \textit{softmax} non-linear
transformation 

\begin{equation}
\tilde{z}_k = f_k(\mathbf{z}) = \frac{\exp(z_k)}{\sum_{k'=0}^{K-1} \exp(z_{k'})}.
\label{eq:softmax}
\end{equation}

Bear in mind that in the next sections we will use $\mathbf{z}$ and
$\tilde{\mathbf{z}}$ to denote the output of linear and non-linear
transformations respectively.

\begin{figure}
\centering
\includegraphics[scale=0.4]{figs/deep_learning/LayerP.pdf}
\caption{Representation of a log-linear model as a computation graph: a
composition of linear and non-linear transformations.}
\label{fig:LayerP}
\end{figure}

\subsection{Stochastic Gradient Descent: a refresher}

As we saw on day one, to determine the parameters of a log linear model
$\Theta=\{\mathbf{W}, \mathbf{b}\}$ Stochastic Gradient Descent (SGD) can be
used. To apply SGD we first need to define an error function that measures how
good we are doing for given parameter values. To remain close to the maximum
entropy example, we will use as cost the average minus posterior probability of
the correct class, also known as the Cross-Entropy (CE) criterion. Bear in
mind, however, that we could pick other non-linear functions and cost functions
that do not have
a probabilistic interpretation. For example, the same principle could be applied to
a regression problem where the cost $\mathcal{F}$ is the Mean Square Error (MSE).

For a training data-set $\mathcal{D}$ of $M$ examples, the CE cost function is given by

\begin{align}
\mathcal{F}(\mathcal{D};\Theta) & = -\frac{1}{M}\sum_{m=0}^{M-1} \log p(y^m_{k(m)} | \mathbf{x}^m).
\label{eq:CostLogPos}
\end{align}
% \nonumber\\&= -\frac{1}{M}\sum_{m=0}^{M-1} (\log \circ f_{k(m)} \circ \mathbf{g})(\mathbf{x}^m^)

\indent Here we use $k(m)$ to denote the index of the correct class for the $m$-th
example. To learn the parameters of this model, all we need is to compute the gradient
of the cost $\nabla\mathcal{F}$ with respect to the parameters of the model and
iteratively update our estimates as 

\begin{equation}
\mathbf{W} \leftarrow \mathbf{W} - \eta \nabla\mathcal{F}_\mathbf{W}. 
\end{equation}

\noindent where $\eta$ is the learning rate, and

\begin{equation}
\mathbf{b} \leftarrow \mathbf{b} - \eta \nabla\mathcal{F}_\mathbf{b}. 
\end{equation}


\subsection{Deriving Gradients in Computation Graphs: The Chain Rule}

Computing $\nabla\mathcal{F}$ is trivial in the case of log-linear models. For
the sake of the introduction to deep learning, we will however show how it can
be done exploiting the decomposition of the cost function into a computational
graph seen in the last section.  

The fundamental mathematical principle behind this approach is the chain rule.
This states the following relation between the derivatives of two functions
$\mathbf{f}$, $\mathbf{g}$ and the derivative of their composition $\mathbf{f}
\circ \mathbf{g}$
%
\begin{align}
\frac{\partial (f_{k} \circ \mathbf{g})(\mathbf{x}) }{\partial w} = \sum_{k'=0}^{K-1}\frac{\partial f_{k}(\mathbf{z})}{\partial z_{k'}}\frac{\partial z_{k'}}{\partial w}.
\label{eq:chainRule}
\end{align}
%
\noindent where 
%
\begin{equation}
z_{k'} = g_{k'}(\mathbf{x})
\end{equation}
%
In other words, to compute the derivative of a complex function we only need
to decompose it one or more times and compute the derivatives of its individual
components. If we look at the gradient
$\nabla_\mathbf{W}\mathcal{F}(\mathcal{D};\Theta)$ for a particular weight
$W_{ki}$, this is given by
%
\begin{equation}
\nabla_\mathbf{W}\mathcal{F}(\mathcal{D};\Theta)_{ki} =-\frac{1}{M}\sum_{m=0}^{M-1} \frac{\partial \log p(y^m_{k(m)} | \mathbf{x}^m) }{\partial W_{ki}}.
\label{eq:gradlogPycx}
\end{equation}
%
In order to compute the gradient we only need to compute each partial
derivative of the cost function, but each of these partial derivatives can be
decomposed using the chain rule in Eq.~\ref{eq:chainRule} as
%
\begin{align}
\frac{\partial \log p(y^m_{k(m)} | \mathbf{x}^m) }{\partial W_{ki}} & = \frac{\partial (\log \circ f_{k(m)} \circ \mathbf{g})(\mathbf{x}^m) }{\partial W_{ki}}\nonumber\\ & = \sum_{k'=0}^{K-1}\frac{\partial \log \circ f_{k(m)}(\mathbf{z}^m)}{\partial z_{k'}}\frac{\partial z_{k'}}{\partial W_{ki}}.
\end{align}
%
now we only need to solve two simpler derivatives. We could actually further
decompose this using the chain rule again but these are easily shown to be
%
\begin{align}
\frac{\partial \log \circ f_{k(m)}(\mathbf{z}^m)}{\partial z_{k}} = 
  \begin{cases}
      1 - \tilde{z}_k^m  &  \mbox{ if } k = k(m)\\ 
      -\tilde{z}_k^m    &  \mbox{ otherwise } 
  \end{cases}. 
  \label{eq:patialSoftmax}
\end{align}
%
\noindent and
\begin{align}
\frac{\partial z_{k'}}{\partial W_{ki}} = 
  &\begin{cases}
      x_i^m  &  \mbox{ if } k = k'\\ 
      0    &  \mbox{ otherwise } 
  \end{cases}.
  \label{eq:partialLinear}
\end{align}

If we plug these two equations into Eq: \ref{eq:gradlogPycx} to obtain each 
derivative for $W_{ki}$ we get the following closed form for the gradient

\begin{equation}
\nabla_\mathbf{W}\mathcal{F}(\mathcal{D};\Theta) = -\frac{1}{M}\sum_{m=0}^{M-1} \Big(\mathrm{\mathbf{1}}_{k(m)} - \tilde{\mathbf{z}}^m \Big) \cdot \left(\mathbf{x}^m\right)^T  
\label{gradWeigths}
\end{equation}

\noindent where $\mathrm{\mathbf{1}}_{k(m)}$ is a vector of zeros with a one in
the position of the correct class for each example $k(m)$. In order to compute
the gradient for the bias $b_{k}$  we only need one additional derivative.

\begin{align}
\frac{\partial z_{k'}}{\partial b_{k}} = 
  &\begin{cases}
      1  &  \mbox{ if } k = k'\\ 
      0  &  \mbox{ otherwise } 
  \end{cases} 
  \label{eqn:eqsilonq}
\end{align}
	
\noindent leading to the gradient

\begin{equation}
\nabla_\mathbf{b}\mathcal{F}(\mathcal{D};\Theta) = -\frac{1}{M}\sum_{m=0}^{M-1} \Big(\mathrm{\mathbf{1}}_{k(m)} - \tilde{\mathbf{z}}^m \Big)   
\label{eq:gradBias}
\end{equation}

\subsection{Changing the non-linearity}

Before we get into deeper models it also useful to revise the case in which our
non-linear function $\mathbf{f}$ is one or more logistic functions

\begin{equation}
\tilde{z}_k = f_k(z_k) = \frac{1}{1+\exp(-z_k)} 
\label{eq:sigmoid}
\end{equation}

which has the following derivative

\begin{equation}
\frac{\partial \tilde{z}_k}{\partial z_k} = \tilde{z}_{k} (1-\tilde{z}_{k}).
\label{eq:partsigmoid}
\end{equation}

\noindent This will be helpful in the future.

\begin{exercise}\label{ex:dl1:mlp_numpy}
Get in contact with the multi-layer perceptron (MLP) class in Numpy and see
that for a single layer this is simply a log-linear model. Revisit the
sentiment classification exercise of day one. Reformulate train and test data
in a way suitable for the exercises of today.  
\begin{python}
import numpy as np
import lxmls.readers.sentiment_reader as srs  
scr     = srs.SentimentCorpus("books")
train_x = scr.train_X.T
train_y = scr.train_y[:, 0]
test_x  = scr.test_X.T
test_y  = scr.test_y[:, 0]
\end{python}
%
Load the MLP and SGD code and create a single layer model by specifying the
number of inputs, outputs and the type of layer. Note that the number of inputs
equals the number of features and the number of outputs the number of classes
(2).
%
\begin{python}
# Define MLP (log linear)
import lxmls.deep_learning.mlp as dl
import lxmls.deep_learning.sgd as sgd
# Model parameters
geometry = [train_x.shape[0], 2]
actvfunc = ['softmax']
# Instantiate model
mlp      = dl.NumpyMLP(geometry, actvfunc)
\end{python}
Put a breakpoint inside of the lxmls/deep\_learning/mlp.py function and debug
step by step. Identify the introduced formulas and the computation of the
gradients (these are to be completed in the next exercise). 
\begin{python}
# Play with the untrained MLP forward
hat_train_y = mlp.forward(train_x) 
hat_test_y  = mlp.forward(test_x) 
# Compute accuracy
acc_train = sgd.class_acc(hat_train_y, train_y)[0]
acc_test  = sgd.class_acc(hat_test_y, test_y)[0]
print("Untrained Log-linear Accuracy train: %f test: %f" % (acc_train, acc_test))
\end{python}
\end{exercise}

\section{Going Deeper than Log-linear by using Composition}

\subsection{The Backpropagation Recursion}

We have seen that just using the chain rule we can easily compute gradients for
compositions of two functions (one non-linear and one linear). However, there
was nothing in the derivation that would stop us from composing more than two
functions. Let's imagine a general case in which we compose $n=1 \cdots N$
pairs of linear and non-linear functions.

\begin{equation}
p(y_k|\mathbf{x}) = (f_k^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^1 \circ \mathbf{g}^1)(\mathbf{x}),
\label{eq:FeedForward}
\end{equation}

\noindent We will denote each linear function as

\begin{equation}
\mathbf{z}^n = \mathbf{g}^n(\tilde{\mathbf{z}}^{n-1}) = \mathbf{W}^n \cdot \tilde{\mathbf{z}}^{n-1} + \mathbf{b}^n 
\end{equation}

\noindent and each non-linear function as 

\begin{equation}
\tilde{\mathbf{z}}^n = \mathbf{f}^n(\mathbf{z}^n).
\end{equation}

Each pair of non-linear and linear transformation, expressed as $(\mathbf{f}^n \circ \mathbf{g}^n)$, can be considered as a \textit{layer}. All the
internal (hidden) layers $\mathbf{f}^1 \cdots \mathbf{f}^{N-1}$ are going to use sigmoid functions as in Eq:
\ref{eq:sigmoid} and $\mathbf{f}^N$ will be a softmax, so that the final output
can be interpreted as a distribution over $K$ classes. The parameters of our
model are going to be the weights and bias of each layer
$\Theta=\{\mathbf{W}^1, \mathbf{b}^1, \cdots \mathbf{W}^N, \mathbf{b}^N\}$.  

If we follow the same steps as in the previous section. Our cost function for
SGD will look like this

\begin{align}
\mathcal{F}(\mathcal{D};\Theta) & = -\frac{1}{M}\sum_{m=0}^{M-1} \log p(y^m | \mathbf{x}^m)\nonumber\\ & = -\frac{1}{M}\sum_{m=0}^{M-1} (\log \circ f_{k(m)}^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^1 \circ \mathbf{g}^1)(\mathbf{x}^m)
\end{align}

If we wanted to compute the gradient for the $n$-th layer, we just need to
apply the chain rule as in the previous section, selecting an intermediate variable to decompose  
and repeating the process until we obtain the final derivative. Fortunately we
do not need to compute this operation each time as it is easy to spot a recursive
rule that is valid for many backpropagation cases, including feed-forward
(MLP) as well as recurrent neural networks (RNN) with minor modifications.  

\noindent Let's consider the split at $\mathbf{z}^n$

%\begin{equation}
%\log p(y^m | \mathbf{x}^m) = (\log \circ f_{k(m)}^N \circ \mathbf{g}^N \circ \cdots \mathbf{f}^{n+1}) \circ (\mathbf{g}^{n+1} \circ \cdots \mathbf{f}^{1} \circ \mathbf{g}^{1})
%\end{equation}
%
%\begin{equation}
%\log p(y^m | \mathbf{x}^m) = (\log \circ f_{k(m)}^N \circ \mathbf{g}^N \circ \cdots \mathbf{f}^{n}) \circ (\mathbf{g}^{n} \circ \cdots \mathbf{f}^{1} \circ \mathbf{g}^{1})
%\end{equation}

Let's start by computing the derivate for an arbitrary $n$-th hidden layer. To
get a similar case as we had for the composition of two transformations, lets
apply the chain rule at the output of the $n$-th linear transformation
$\mathbf{z}^n$ to split. Since anything before th $n$-th layer does not depend
on $W_{ji}^n$ this looks like  

\begin{align}
\frac{\partial \log p(y^m | \mathbf{x}^m)}{\partial W_{ji}^n} & = \sum_{j'=1}^J \frac{\partial}{\partial z^n_{j'}} (\log \circ f_{k(m)}^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^{n+1} \circ \mathbf{g}^{n+1} \circ \mathbf{f}^{n})(\mathbf{z}^{nm})\frac{\partial z^n_{j'}}{\partial W_{ji}^n}\nonumber\\ & = \sum_{j'=1}^J e^{n}_{j'} \frac{\partial z^n_{j'}}{\partial W_{ji}^n}\nonumber\\ & = e^{n}_{j'}\frac{\partial z^n_{j}}{\partial W_{ji}^n}
\label{eq:partialfn}
\end{align}

where the last equality uses the fact that $z^n_{j'}$ only depends on
$W_{j'i}^n$ if $j=j'$ and thus the sum over $j'$ disappears. The term $e^{n}_j$
represents the derivative of the cost with respect to an intermediate output of
a linear transformation $\mathbf{z}^n$ (for the $m$-th example)

\begin{equation}
e^{n}_j = \frac{\partial}{\partial z^{n}_{j}} (\log \circ f_{k(m)}^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^{n+1} \circ \mathbf{g}^{n+1} \circ \mathbf{f}^{n})(\mathbf{z}^{mn}) 
\end{equation}

\noindent If we consider the same derivative one layer upwards in the
computation graph  

\begin{equation}
e^{n+1}_k = \frac{\partial}{\partial z^{n+1}_{k}} (\log \circ f_{k(m)}^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^{n+1})(\mathbf{z}^{m(n+1)})
\end{equation}

\noindent we can apply the chain rule twice to obtain the
recursion\footnotemark\footnotetext{See the annex at the end of this chapter
for a detailed derivation}

\begin{equation}
e^{n}_j = \sum_{k=1}^K e^{n+1}_k \frac{\partial z^{n+1}_k}{\partial \tilde{z}_{j}^n}\frac{\partial \tilde{z}^n_{j}}{\partial z_{j}^n} 
\label{eq:chainRulRecursion}
\end{equation}

\begin{figure}
\centering
\includegraphics[scale=0.4]{figs/deep_learning/LayerP2.pdf}
\caption{Detail of propagation of error in weight $W_{ji}$ from layer $n$ to
layer $n+1$. Note that while only $z_j^n$ is affected by $W_{ji}$ all outputs
of layer $n+1$ are affected by changes in $W_{ji}$.}
\label{fig:LayerP2}
\end{figure}

\noindent It only rests to compute the derivative

\begin{equation}
\frac{\partial z^{n+1}_k}{\partial \tilde{z}_{j}^n} = W_{kj}^{n+1} 
\end{equation}

\noindent and use the derivative of the sigmoid, given in Eq:
\ref{eq:partsigmoid}. This leads to the following recursion for the derivative
with respect to $W_{ji}$

\begin{equation}
e^{n}_j = \sum_{k=1}^K e^{n+1}_k W_{kj}^{n+1}\tilde{z}^n_{j}(1-\tilde{z}^n_{j})
\end{equation}

\noindent where $e^{N}_j$ is given in Eq.~\ref{eq:patialSoftmax}. If we express this in matrix form we get the backpropagation rule which we can integrate into the SGD recursion, see Algorithm \ref{algo:backprop}. 

\begin{algorithm}[t]
\label{algo:backprop}

   \caption{Mini-batch SGD with Back-Propagation \label{alg:maxent_gd}}

\begin{algorithmic}[1]

   \STATE {\bfseries input:} Data $\mathcal{D}$, Feed-forward of $N$ layers, with parameters $\Theta=\{\mathbf{W}^1, \mathbf{b}^1, \cdots \mathbf{W}^N, \mathbf{b}^N\}$, number of rounds $T$, $B$ mini-batches of size $M$, learning rate $\eta$

   \STATE initialize parameters $\Theta$ randomly 

	\FOR{$t=1$ {\bfseries to} $T$}
	\FOR{$b=1$ {\bfseries to} $B$}
	\STATE Compute the forward pass for the $M$ examples in batch $b$ :
    $$p(y_k^m|\mathbf{x}^m) \equiv \tilde{\mathbf{z}}^{mN} = (f_k^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^1 \circ \mathbf{g}^1)(\mathbf{x}^m),$$

	\STATE keep all intermediate non-linear outputs $\tilde{\mathbf{z}}^{mN} \cdots \tilde{\mathbf{z}}^{m1}$

	\FOR{$n=N$ {\bfseries to} $1$}
        \IF{n==N} 
        \STATE {Initialize error with error at last layer 
        $$\mathbf{e}^N = \Big(\mathrm{\mathbf{1}}_{k(m)} - \tilde{\mathbf{z}}^{mN} \Big)$$}
        \ELSE
	    \STATE propagate the error backward one layer:  
        $$\mathbf{e}^{n} = \Big((\mathbf{W}^{n+1})^T \mathbf{e}^{n+1}\Big) \odot \tilde{\mathbf{z}}^n \odot (\mathbf{\mathrm{1}}-\tilde{\mathbf{z}}^n)$$

        $\odot$ is the element-wise product and the $\mathbf{\mathrm{1}}$ is replicated to match the size of $\tilde{\mathbf{z}}^n$
        \ENDIF 

        \STATE Compute the gradients from the errors and the inputs

        $$\nabla_{\mathbf{W}^n}\mathcal{F}(\mathcal{D};\Theta)  = -\frac1M \sum_{m=0}^{M-1} \mathbf{e}^{n} \cdot \left(\tilde{\mathbf{z}}^{n-1}\right)^T$$ 
        $$\nabla_{\mathbf{b}^n}\mathcal{F}(\mathcal{D};\Theta)  = - \frac1M \sum_{m=0}^{M-1} \mathbf{e}^{n}$$  

        \STATE Update the parameters 
            $$\mathbf{W}^n \leftarrow \mathbf{W}^m - \eta \nabla\mathcal{F}_\mathbf{W^n}$$ 
            $$\mathbf{b}^n \leftarrow \mathbf{b}^n - \eta \nabla\mathcal{F}_\mathbf{b^n}$$  

	\ENDFOR

	\ENDFOR
	\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{exercise}\label{ex:dl1:grads}
Go to \texttt{lxmls/deep\_learning/mlp.py:class NumpyMLP:def grads()} and complete the
code of the NumpyMLP class with the Backpropagation recursion that we just saw.
Once you are done, try different network geometries by increasing the number of
layers and layer sizes, e.g.:
\begin{python}
# Model parameters
geometry = [train_x.shape[0], 20, 2]
actvfunc = ['sigmoid', 'softmax'] 
# Instantiate model
mlp      = dl.NumpyMLP(geometry, actvfunc) 
\end{python}
You can test the different models with the same sentiment analysis problem as
in Exercise \ref{ex:dl1:mlp_numpy}.
\begin{python}
# Model parameters
n_iter = 5
bsize  = 5
lrate  = 0.01
# Train
sgd.SGD_train(mlp, n_iter, bsize=bsize, lrate=lrate, train_set=(train_x, train_y))
acc_train = sgd.class_acc(mlp.forward(train_x), train_y)[0]
acc_test  = sgd.class_acc(mlp.forward(test_x), test_y)[0]
print("MLP %s Model Amazon Sentiment Accuracy train: %f test: %f" % (geometry, acc_train, acc_test))
\end{python}
\end{exercise}


\subsection{Some final reflections on Backpropagation}

If you are new to the neural network topic, this is about the most important
piece of theory you should learn about deep learning. Here are some reflections
that you should keep in mind.

\begin{itemize}
\item Thanks to the multi-layer structure and the chain rule, Backpropagation allows models that compose linear and non-linear functions with any depth (in principle\footnotemark). 

\item The formulas are also valid for other cost functions and output layer non-linearities with minor modifications. It is only necessary to compute the equivalente of Eq.~\ref{eq:patialSoftmax}. 

\item The formulas are also valid for hidden non-linearities other than the sigmoid. Element-wise non-linear transformations still allow the simplification in Eq: \ref{eq:chainRulRecursion}. With little effort it is also possible to deal with other cases.

\item However, there is an important limitation: Unlike the log-linear models, the optimization problem is \textit{non convex}. This removes some formal guarantees, most importantly we can get trapped in local minima during training.
\end{itemize}

\footnotetext{Not exactly, since it is possible to run into numerical problems.}

% THIS WILL BE COMMENTED UNTIL THE FINAL UPDATE
%
%\subsection{A Note on Pre-Trainining}
%
%If you already have some experience with neural networks you might have
%realised that all what we show here is classic MLP theory, which is 30-40 years
%old. Indeed, it could be argued that, at the core, many modern deep learning
%applications are just classical neural networks theory with more data and more
%computing power. One of the novelties that came along the deep learning wave of
%research is pre-training with the Restricted Boltzmann Machine (RBM) paradigm.
%The cost function for a MLP of many layers becomes too complex and, as
%mentioned before, local minima and non-convexity make training of deep MLPs
%problematic. 
%
%The RBM paradigm allows to pre-train a MLP in \textit{unsupervised} fashion
%i.e. with out reference labels $\mathbf{y}^m$, which has been shown to improve
%posterior training. However, current state of the art systems do not always use
%RBM pre-training, resort to simpler types of smart-initializations or use none.
%Fort his reason, we will only see pre-training as an advance topic.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Deriving gradients and GPU code with Theano}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{An Introduction to Theano}

As you may have observed, the speed of SGD training for MLPs slows down
considerably when we increase the number of layers. One reason for this is that
the code that we use here is not very optimized. It is thought for you to
learn the basic principles. Even if the code was more optimized, it would still be
very slow for reasonable network sizes. The cost of computing each
linear layer is proportional to the dimensionality of the previous and current
layers, which in most cases will be rather large. 

For this reason most deep learning applications use Graphics Processing Units
(GPU) in their computations. This specialized hardware is normally used to
accelerate computer graphics, but can also be used for general computation
intensive tasks. However, we need to deal with specific interfaces and
operations in order to use a GPU. This is where Theano comes in. Theano is a
multidimensional symbolic expression python module with focus on neural
networks. It will provide us with the following nice features:

\begin{itemize}
\item Symbolic expressions: Express the operations of the MLP (forward pass, cost) symbolically, as mathematical operations rather than explicit code 
\item Symbolic Differentiation: As a consequence of the previous feature, we can compute gradients of arbitrary mathematical functions automatically.   
\item GPU integration: The code will be ready to work on a GPU, provided that you have one and it is active within Theano. It will also be faster on normal CPUs since the symbolic operations are compiled to C code. 
\item Theano is focused on Deep Learning, with an active community and several tutorials easily available.  
\end{itemize}

The only negative aspect is that we will have to learn to deal with Theano and
in particular working with symbolic representations. We will start right away
with some exercises.

\begin{exercise}\label{ex:dl1:get_in_contact}
Get in contact with Theano. Learn the difference between a symbolic
representation and a function. Start by implementing the first layer of our
previous MLP in Numpy
\begin{python}
# Numpy code
x = test_x  # Test set
W1, b1 = mlp.params[0:2]  # Weights and bias of fist layer
z1 = np.dot(W1, x) + b1  # Linear transformation
tilde_z1 = 1 / (1 + np.exp(-z1))  # Non-linear transformation
\end{python}
Now we will implement this in Theano.  We start by creating the variables over
which we will produce the operations. For example the symbolic input is defined
as
\begin{python}
# Theano code. 
# NOTE: We use underscore to denote symbolic equivalents to Numpy variables.
# This is no Python convention!
import theano
import theano.tensor as T

_x = T.matrix('x')
\end{python}
Note that this variable does not have any particular value, nor a space
reserved in memory for it. It contains just a symbolic definition of what the
variable can contain. The particular values will be given when we use it to
compile a function. 

We could actually use the same definition format to define the weights and give
their particular values as inputs to the compiled function. However, since we
will be using a more complicated format in later exercises, we will use it here
as well. The \textit{shared} class allows to define variables that are shared
across functions. They are also given a concrete value so that we do not need
to give it for each function call. This format is therefore ideal for the
weights of our network.
\begin{python}
_W1 = theano.shared(value=W1, name='W1', borrow=True)
_b1 = theano.shared(value=b1, name='b1', borrow=True, broadcastable=(False, True))
\end{python}
Now lets describe the operations we want to do with the variables. Again only
symbolically. This is done by replacing our usual operations by Theano symbolic
ones when necessary e. g. the internal product dot() or the sigmoid. Some
operations like e.g. $+$ are automatically recognized by Theano (operator
overloading). 
\begin{python}
_z1 = T.dot(_W1, _x) + _b1
_tilde_z1 = T.nnet.sigmoid(_z1)
# Keep in mind that naming variables is useful when debugging
_z1.name = 'z1'
_tilde_z1.name = 'tilde_z1'
\end{python}
When debugging the code it is often useful to print the graph of computations.
\begin{python}
# Perceptron computation graph
theano.printing.debugprint(_tilde_z1)
\end{python}

\begin{lstlisting}
sigmoid [@A] 'tilde_z1'
 |Elemwise{add,no_inplace} [@B] 'z1'
   |dot [@C] ''
   | |W1 [@D]
   | |x [@E]
   |b1 [@F]
\end{lstlisting}

It is important to keep in mind that, until this point, we do not have a
function we can use to produce any practical input. In order to obtain this we
have to compile this function by calling    
\begin{python}
layer1 = theano.function([_x], _tilde_z1)
\end{python}
Note the use of $[$ $]$ for the input variables, even if we just specify one
variable. We can now do a test to compare the Numpy and Theano implementations
and see that they give the same outputs.
\begin{python}
# Check Numpy and Theano match
if np.allclose(tilde_z1, layer1(x.astype(theano.config.floatX))):
    print("\nNumpy and Theano Perceptrons are equivalent")
else:
    raise ValueError("Numpy and Theano Perceptrons are different")
\end{python}
\end{exercise}

\subsection{Symbolic Forward Pass}
In the previous section you have seen how to create symbolic Theano functions
with shared parameters. You have thus all you need to implement the whole
forward pass of a generic MLP in Theano.
\begin{exercise}
Complete the method \_forward() inside of the lxmls/deep\_learning/mlp.py:class
TheanoMLP. Note that this is called only once at the initialization of the
class. To debug your implementation put a breakpoint at the \_\_init\_\_
function call. Hint: Note that this is very similar to NumpyMLP.forward().
You just need to keep track of the symbolic variable representing the output of
the network after each layer is applied and compile the function at the end.
After you are finished instantiate a Theano class and check that Numpy and
Theano forward pass are the same. 

\begin{python}
mlp_a = dl.NumpyMLP(geometry, actvfunc)
mlp_b = dl.TheanoMLP(geometry, actvfunc)
\end{python}

Bear in mind that you can use previous experience to debug this.

\end{exercise}

\subsection{Symbolic Differentiation}
In the previous section we compiled the forward pass of a MLP. In this section
we will do the same with the cost used for training. We will also derive the
gradients although this will be trivial once we have the cost function compiled.     
\begin{exercise}
We first see an example that does not use any of the code in TheanoMLP but
rather continues from what you wrote in exercise \ref{ex:dl1:get_in_contact}. In this exercise you
completed a sigmoid layer with Theano. To get some values for the weights we
used the first layer of the network you trained in \ref{ex:dl1:grads}. now we are going to use
the second layer as well. This is thus assuming that your network in \ref{ex:dl1:grads} has
only two layers e.g. the recommended geometry (I, 20, 2). Make sure this is the
case before starting this exercise.  

For the sake of clarity, lets write here the part of Ex. \ref{ex:dl1:grads} that we had completed
\begin{python}
# Get the values from our MLP from Ex 5.2
W1, b1   = mlp.params[:2]     # Weights and bias of fist layer 
# First layer symbolic variables
_x  = T.matrix('x')
_W1 = theano.shared(value=W1, name='W1', borrow=True) 
_b1 = theano.shared(value=b1, name='b1', borrow=True, broadcastable=(False, True)) 
# First layer symbolic expressions
_z1       = T.dot(_W1, _x) + _b1
_tilde_z1 = T.nnet.sigmoid(_z1)
\end{python}
Now we just need to complete this with the second layer, using a softmax non-linearity
\begin{python}
W2, b2 = mlp.params[2:]  # Weights and bias of second (and last!) layer

# Second layer symbolic variables
_W2 = theano.shared(value=W2, name='W2', borrow=True)
_b2 = theano.shared(value=b2, name='b2', borrow=True, broadcastable=(False, True))

# Second layer symbolic expressions
_z2 = T.dot(_W2, _tilde_z1) + _b2
_tilde_z2 = T.nnet.softmax(_z2.T).T
\end{python}
With this, we could compile a function to obtain the output of the network
symb\_tilde\_z2 for a given input symb\_x. In this exercise we are however
interested in obtaining the misclassification cost. This is given in Eq:
\ref{eq:CostLogPos}. First we are going to need the symbolic variable for the
correct output
\begin{python}
_y = T.ivector('y')
\end{python}
The minus posterior probability of the class given the input is the same as
selecting the $k(m)$-th softmax output, were $k(m)$ is the index of the correct
class for $x^m$. If we want to do this for a vector $\mathbf{y}$ containing $M$
different examples, we can write this as
\begin{python}
_F = -T.mean(T.log(_tilde_z2[_y, T.arange(_y.shape[0])]))
\end{python}
Now obtaining a function that computes the gradient could not be easier.
\begin{python}
_nabla_F = T.grad(_F, _W1)
nabla_F = theano.function([_x, _y], _nabla_F)
\end{python}
To finish this exercise have a look at the TheanoMLP class. As you may realise it just implements what is shown above for the generic case of $N$ layers
\end{exercise}

\subsection{Symbolic mini-batch update}

The code above is used in the normal SGD\_train when utilizing Theano. Even if
you do not have a GPU configured, it should be run faster than our Numpy
version, particularly for large batch sizes. There is however a
way to make it run even faster by implementing not only gradient computation
but the whole batch update of SGD inside Theano. For this we need also to share
the whole training set, or a very large mega-batch of it. 

\begin{exercise}
Let's first have an understanding of handling train/test data inside the Theano
computation graph. One important aspect to take into account is that both type
and shape of the data have to match their corresponding graph variables. This is
the main source of errors when you are starting with Theano. 
\begin{python}
# Cast data into the types and shapes used in the Theano graph
train_x = train_x.astype(theano.config.floatX)
train_y = train_y.astype('int32')
\end{python}
Note the Theano type theano.config.floatX. This will automatically switch
between float32 (GPU) and float64 (CPU).

To use data in a Theano computation graph, we use the theano.shared variable.
This will also push data into the GPU, if used.
\begin{python}
_train_x = theano.shared(train_x, 'train_x', borrow=True)
_train_y = theano.shared(train_y, 'train_y', borrow=True)
\end{python}
Once this is done, we can create and compile functions using these variables.
One simple but useful function is a function returning a mini-batch of data 
of size bsize given the mini-batch index.  
\begin{python}
_i = T.lscalar()
get_tr_batch_y = theano.function([_i], _train_y[_i * bsize:(_i + 1) * bsize])
\end{python}
Compile this function and observe its behaviour. It will be necessary for the next exercise.
\end{exercise}

\begin{exercise}
The mini-batch function in the previous exercise is the key to fast batch
update. This is combined with the \emph{updates} argument of theano.function. The input to this argument,
is a list of tuples with each parameter and update rule. This can be compactly
defined using list comprehensions.
\begin{python}
mlp_c   = dl.TheanoMLP(geometry, actvfunc)
_x      = T.matrix('x')
_y      = T.ivector('y')
_F      = mlp_c._cost(_x, _y)
updates = [(par, par - lrate*T.grad(_F, par)) for par in mlp_c.params]
\end{python}

This can be now combined with the givens argument of theano.function. This maps
input and target to other variables. In this case a mini-batch of inputs and
targets given an index. 
\begin{python}
_j      = T.lscalar()
givens  = { _x : _train_x[:, _j*bsize:(_j+1)*bsize], 
            _y : _train_y[_j*bsize:(_j+1)*bsize] }
\end{python}

With updates and givens, we can now define the batch update function. This will
return the cost of each batch and update the MLP parameters at the same time
using updates
\begin{python}
batch_up = theano.function([_j], _F, updates=updates, givens=givens)
n_batch  = train_x.shape[1]/bsize  + 1
\end{python}
Once we have defined this, we can compare speed and accuracy of the Numpy
and simple gradient versions using

\begin{python}
import time
# Model
geometry = [train_x.shape[0], 20, 2]
actvfunc = ['sigmoid', 'softmax'] 

# Numpy MLP
mlp_a     = dl.NumpyMLP(geometry, actvfunc)
init_t    = time.clock()
sgd.SGD_train(mlp_a, n_iter, bsize=bsize, lrate=lrate, train_set=(train_x, train_y))
print "\nNumpy version took %2.2f sec" % (time.clock() - init_t)
acc_train = sgd.class_acc(mlp_a.forward(train_x), train_y)[0]
acc_test  = sgd.class_acc(mlp_a.forward(test_x), test_y)[0]
print "Amazon Sentiment Accuracy train: %f test: %f\n" % (acc_train, acc_test)

# Theano grads 
mlp_b  = dl.TheanoMLP(geometry, actvfunc)
init_t = time.clock()
sgd.SGD_train(mlp_b, n_iter, bsize=bsize, lrate=lrate, train_set=(train_x, train_y))
print "\nCompiled gradient version took %2.2f sec" % (time.clock() - init_t)
acc_train = sgd.class_acc(mlp_b.forward(train_x), train_y)[0]
acc_test  = sgd.class_acc(mlp_b.forward(test_x), test_y)[0]
print "Amazon Sentiment Accuracy train: %f test: %f\n" % (acc_train, acc_test)

# Theano batch update
init_t = time.clock()
sgd.SGD_train(mlp_c, n_iter, batch_up=batch_up, n_batch=n_batch)
print "\nTheano compiled batch update version took %2.2f" % (time.clock() - init_t)
acc_train = sgd.class_acc(mlp_c.forward(train_x), train_y)[0]
acc_test  = sgd.class_acc(mlp_c.forward(test_x), test_y)[0]
print "Amazon Sentiment Accuracy train: %f test: %f\n"%(acc_train,acc_test)
\end{python}
As you may observe, just computing the gradients with Theano may not lead to
a decrease, but rather an increase in computing time. To maximally exploit
the power of Theano, it is necessary to bundle both computations and data 
together using approaches like the compiled batch update.
\end{exercise}

\section*{Annex: Backpropagation in Detail}

The objective of this section is to demonstrate that, for the feed-forward
network defined in Eq.~\ref{eq:FeedForward}, if we define the derivatives of
the error with respect to to adjacent linear layer outputs
$\mathbf{z}^{m(n+1)}$ and $\mathbf{z}^{mn}$, given by

\begin{equation}
e^{n}_j = \frac{\partial}{\partial z^{n}_{j}} (\log \circ f_{k(m)}^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^{n+1} \circ \mathbf{g}^{n+1} \circ \mathbf{f}^{n})(\mathbf{z}^{mn}) 
\label{eq:endetail}
\end{equation}

\noindent and 

\begin{equation}
e^{n+1}_k = \frac{\partial}{\partial z^{n+1}_{k}} (\log \circ f_{k(m)}^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^{n+1})(\mathbf{z}^{m(n+1)})
\label{eq:enp1detail}
\end{equation}

\noindent then for a feed-forward network with softmax output non-linearity and one-to-one intermediate non-linearities, the following relation holds. 

\begin{equation}
e^{n}_j = \sum_{k=1}^K e^{n+1}_k \frac{\partial z^{n+1}_k}{\partial \tilde{z}_{j}^n}\frac{\partial \tilde{z}^n_{j}}{\partial z_{j}^n} 
\label{eq:DetailchainRulRecursion}
\end{equation}

\noindent We departure from

\begin{equation}
e^{n}_j = \frac{\partial}{\partial z^{n}_{j}} (\log \circ f_{k(m)}^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^{n+1} \circ \mathbf{g}^{n+1} \circ \mathbf{f}^{n})(\mathbf{z}^{mn}) 
\end{equation}

\noindent We now apply the chain rule at the non-linearity $\tilde{\mathbf{z}}^{mn}$.

\begin{equation}
e^{n}_j =  \sum_{j'=0}^{J-1}\frac{\partial}{\partial \tilde{z}^{n}_{j'}} (\log \circ f_{k(m)}^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^{n+1} \circ \mathbf{g}^{n+1})(\tilde{\mathbf{z}}^{mn})\frac{\partial \tilde{z}^n_{j'}}{\partial z_{j}^n}.
\end{equation}

Since this entails various one-to-one non-linear transformations, all
derivatives will be zero when $j\neq j'$, thus yielding 

\begin{equation}
e^{n}_j =  \frac{\partial}{\partial \tilde{z}^{n}_{j}} (\log \circ f_{k(m)}^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^{n+1} \circ \mathbf{g}^{n+1})(\tilde{\mathbf{z}}^{mn})\frac{\partial \tilde{z}^n_{j}}{\partial z_{j}^n}.
\end{equation}

We now apply the chain rule at the linear output $\mathbf{z}^{m(n+1)}$. In this case the sum in the chain rule does not go away. It is easy to
understand why by looking at Figure \ref{fig:LayerP2}, the weight $W_{kj}$
contributes not only to the variable $\tilde{z}^n_j$ but to all variables of the
next linear layer $z^{n+1}_k$, and thus to the possible error. 

\begin{equation}
e^{n}_j = \sum_{k=1}^K \frac{\partial}{\partial z^{n+1}_{k}} (\log \circ f_{k(m)}^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^{n+1})(\mathbf{z}^{m(n+1)})\frac{\partial z^{n+1}_k}{\partial \tilde{z}_{j}^n}\frac{\partial \tilde{z}^n_{j}}{\partial z_{j}^n}.
\label{eq:partialfn4}
\end{equation}

\noindent By looking at Eqs: \ref{eq:enp1detail} and \ref{eq:enp1detail} we can easily
spot the recursion in Eq.~\ref{eq:DetailchainRulRecursion}.  


