\section{Today's assignment}
Today's class will be divided in two main parts. First, we will learn the main 
algorithmic principle behind deep learning (the Backpropagation algorithm) and code it in Numpy. Second, we will learn the basics of the Theano module for Python, 
which allows to easily implement deep learning techniques that run on a 
Graphical Processing Unit (GPU) for optimal speed. 

If you are new to the topic you should aim to finish the deep learning 
principles part (Ex. 6.1 and 6.2) and at least Ex 6.3 and 6.4 of the Theano 
part. If you already know Backpropagation well and have experience with normal 
Python, you should aim to complete the whole day. 

\section{Introduction to Deep Learning and Theano}

Deep learning is the name behind the latest wave of successful neural network 
research, a very old topic dating from the first half of the 20th century. Deep 
learning techniques have attained formidable impact in the machine learning 
community. 
Some of the changes that led to this renewed success are, not only
improvements and insight on the existing neural network algorithms, but also
the increase in the amount of data available and computing power. In
particular, the use of Graphical Processing Units (GPUs) has allowed neural
networks to be applied to very large datasets. Working with GPUs is not trivial
as it requires dealing with specialized hardware. Luckily, as it is often the
case, we are one Python import away from solving this problem. For the
particular case of deep learning, the
Theano\footnotemark\footnotetext{http://deeplearning.net/software/theano/}
module allows us to express models symbolically, automatically computing
gradients and solve overflow problems. Furthermore, the code is also ready to
use with CUDA-compatible GPUs.

There is nothing particularly difficult in the deep learning concept. You have
already visited all the mathematical principles you need in the first days of the
labs of this school. At their core, deep learning models are just functions
mapping vector inputs $\mathbf{x}$ to vector outputs $\mathbf{y}$, just like any
of the models we saw on previous days. Deep learning models are constructed by
composing linear and non-linear transformations to build structures that vaguely
resemble human neural networks, hence the name artificial neural networks. Due
to their compositional nature, gradient methods and the chain rule are used to
learn the parameters of these models. See Section \ref{unsupervised} for a
refresh on the basic concept \mla{which basic concepts/section are you referring to? Section (2.7) is "unsupervised learning of HMM"...}. We will also refer to the gradient
learning methods introduced in Section \ref{s:me}.

%\mla{TODO: dizer algo sobre feed-woard networks, vs cyclic NN such as RNN that will be treated in the next day}
Today we will focus on \textit{feed-forward networks}. These were the first and simplest neural network models \mla{add reference?}, which are characterized by the absence of cycles. Tomorrow, we will extended 
% the content of 
today's class to a well-known and powerful type of cyclic neural networks: the \textit{recurrent neural networks} (RNNs). 

\section{Non-Linear functions as computation graphs} 

\subsection{Example: The computation graph of a log-linear model}

%The most basic architecture used in deep learning is the feed forward network,
%also known as Multi-Layer Perceptron (MLP). This is a non-linear model built by
%alternatively composing linear and non-linear transformations. The log-linear
%models we saw on Day 1 can be interpreted as a special case of MLPs with just
%one layer and a soft-max non-linearity. For this reason, we will use them as an
%initial example to introduce the concept, and progress from there on.

Recall the maximum entropy classifier in Section \ref{s:me}. This was a log-linear model Eq.\ref{eq:loglinear} that, given an input vector of features $\boldsymbol{f}(x,y)$, would
predict the probability of that vector belonging to each of $K$ possible
classes $c_k$. %$y_k$.
 For the purpose of introducing deep learning, 
 we will use a very similar model that is given by
% we will use a more general form given by 
%
\begin{align}
p(y=c_k|{x}) & = \frac{1}{Z}\exp(\mathbf{w}_k \cdot \mathbf{x} + b_k),
\label{eq:loglineargen}
\end{align}
%
%where we simply denote the continuous features of the input $x$ by the vector $\mathbf{x}$. the partition function $Z$ is given by
%
where the input $x$ is represented by the feature vector $\mathbf{x} \in \mathbb{R}^{I}$; $\mathbf{w}_k \in \mathbb{R}^{I}$ is a column vector with the elements in the $k$-{th} row of a weighting parameter matrix $\textbf{W} \in \mathbb{R}^{I \times K}$ and $b_k$ it the $k$-th element of a bias parameter vector $\textbf{b} \in \mathbb{R}^{K}$; finally, $Z$ is the partition function:
\begin{align}
Z(\mathbf{W},\mathbf{b},\mathbf{x}) = \sum_{k'=1}^{K} \exp(\mathbf{w}_{k'} \cdot \mathbf{x} + b_{k'}). 
\label{eq:loglineargenPartition}
\end{align}
%
The present model has small differences with respect to  Eq.\ref{eq:loglinear}. In Eq.\ref{eq:loglineargen}, the model
parameters $\Theta=\{\mathbf{W}, \mathbf{b}\}$ include not only weights $\mathbf{W} \in \mathbb{R}^{K \times J}$ but also a bias $\mathbf{b} \in
\mathbb{R}^{K}$. Also, $\mathbf{x} \in \mathbb{R}^{I}$ is a
generic input vector of real valued features of dimension $I$ instead of a vector of binary features $f(x,y)$. If we consider the binary
joint feature mapping $\boldsymbol{f}(x,y) = \boldsymbol{g}(x) \otimes \boldsymbol{e}_y\nonumber$ of Eq.\ref{eq:jointfeatsimple}, the maximum entropy classifier in Eq.\ref{eq:loglinear}
%can be considered
becomes a special case of Eq.\ref{eq:loglineargen}, in which the feature vector $\mathbf{x}$ only take binary values and the bias parameters in $\mathbf{b}$ are all set to zero.

The mode in  Eq.\ref{eq:loglineargen} can be seen as a composition of
linear and non-linear transformations, as the displayed in the computation graph of Fig.\ref{fig:LayerP}:

\begin{equation}
p(y=c_k|{x}) \equiv \tilde{z}_k = (f_k \circ \mathbf{g})(\mathbf{x}),
\end{equation}
%
where $\circ$ indicates function composition; the intermediate vector variable $\mathbf{z} \in \mathbb{R}^{K}$ is given by a linear transformation of the input vector $\mathbf{x}$

\begin{equation}
\mathbf{z} = \mathbf{g}(\mathbf{x}) = \mathbf{W} \cdot \mathbf{x} + \mathbf{b},
\label{eq:linear}
\end{equation}
%
which is then transformed through the \textit{softmax} non-linear
transformation

\begin{equation}
\tilde{z}_k = f_k(\mathbf{z}) = \frac{\exp(z_k)}{\sum_{k'=1}^{K} \exp(z_{k'})}.
\label{eq:softmax}
\end{equation}

Bear in mind that, similarly to what happened in this section, in the next sections we will denote $\mathbf{z}$ and
$\tilde{\mathbf{z}}$ as the output of linear and non-linear
transformations, respectively.

\begin{figure}
\centering
\includegraphics[scale=0.4]{figs/deep_learning/LayerP.pdf}
\caption{Representation of a log-linear model as a computation graph: a
composition of linear and non-linear transformations. \mla{TODO: update figure.}}
\label{fig:LayerP}
\end{figure}

\subsection{Stochastic Gradient Descent: a refresher}

As we saw on day one, the parameters of a log linear model
$\Theta=\{\mathbf{W}, \mathbf{b}\}$ can be learned with Stochastic Gradient Descent (SGD). To apply SGD we first need to define an error function that measures how
good we are doing for given parameter values. %.
 To remain close to the maximum
entropy example, we will use as cost the average minus posterior probability of
the correct class, also known as the Cross-Entropy (CE) criterion. Bear in
mind, however, that we could pick other non-linear functions and cost functions
that do not have
a probabilistic interpretation. For example, the same principle could be applied to
a regression problem where the cost is the Mean Square Error (MSE).
%
For a training data-set $\mathcal{D} = \{(x^1,y^1), \ldots, (x^M,y^M)\}$ of $M$ examples, the CE cost function is given by

\begin{align}
\mathcal{F}(\mathcal{D};\Theta) 
& = -\frac{1}{M}\sum_{m=1}^{M} \log p(y^m=c_{k(m)} | \mathbf{x}^m) \\
& = -\frac{1}{M}\sum_{m=1}^{M} \log \tilde{{z}}^{k(m)} ,
\label{eq:CostLogPos}
\end{align}
% \nonumber\\&= -\frac{1}{M}\sum_{m=0}^{M-1} (\log \circ f_{k(m)} \circ \mathbf{g})(\mathbf{x}^m^)
%
where $y^m=c_{k(m)}$ is the correct class for the $m$-th
example and $k(m)$ is the index of that class.
%\indent Here we use $k(m)$ to denote the index of the correct class for the $m$-th example. 
Finally, to learn the parameters of this model, we need is to compute the gradient
of the cost $\nabla\mathcal{F}$ with respect to the parameters of the model and
iteratively update our estimates as 

\begin{equation}
\mathbf{W} \leftarrow \mathbf{W} - \eta \nabla_\mathbf{W}\mathcal{F}
\end{equation}
and
\begin{equation}
\mathbf{b} \leftarrow \mathbf{b} - \eta \nabla_\mathbf{b}\mathcal{F},
\end{equation}

\noindent where $\eta$ is the learning rate.



\subsection{Deriving Gradients in Computation Graphs: The Chain Rule}

The expressions for $\nabla\mathcal{F}$ are well know in the case of log-linear models. However, for
the sake of the introduction to deep learning we will show how it can
be done by exploiting the decomposition of the cost function into the computational
graph seen in the last section (and represented in Fig. \ref{fig:LayerP}).  

The mathematical principles behind this approach are the derivative rules, in articular the chain rule. 
%, which related the derivative of two function with their composition.
The chain rule states that the derivative of a composed function $({f_k} \circ \mathbf{g})(\mathbf{x}) : \mathbb{R}^{I} \rightarrow \mathbb{R}$ can be written as a function of the derivatives of each individual functions $\mathbf{g}(\mathbf{x}): \mathbb{R}^{I} \rightarrow \mathbb{R}^{K}$ and ${f_k(\mathbf{z})}: \mathbb{R}^{K} \rightarrow \mathbb{R}$, as follows
%This states the following relation between the derivatives of two functions
%$\mathbf{f}$, $\mathbf{g}$ and the derivative of their composition $\mathbf{f}
%\circ \mathbf{g}$
%
\begin{align}
\frac{\partial (f_{k} \circ \mathbf{g})(\mathbf{x}) }{\partial w} = \sum_{k'=1}^{K}\frac{\partial f_{k}(\mathbf{z})}{\partial z_{k'}}\frac{\partial z_{k'}}{\partial w},
\label{eq:chainRule}
\end{align}
%
where 
%
\begin{equation}
z_{k'} = g_{k'}(\mathbf{x}).
\end{equation}
%
In other words, to compute the derivative of a complex function we only need
to decompose it in several simpler function and compute the derivatives of those individual components. 

Let us consider the entrance $(i,j)$ of the gradient
$\nabla_\mathbf{W}\mathcal{F}(\mathcal{D};\Theta)$, which contain the partial derivative with respect to the weight $w_{ki}$
%
\begin{equation}
\frac{\partial \mathcal{F}(\mathcal{D};\Theta)}{\partial w_{ki}} =-\frac{1}{M}\sum_{m=1}^{M} \frac{\partial \log p(y^m | \mathbf{x}^m) }{\partial w_{ki}}.
\label{eq:gradlogPycx}
\end{equation}
%\begin{equation}
%\nabla_\mathbf{W}\mathcal{F}(\mathcal{D};\Theta)_{ki} =-\frac{1}{M}\sum_{m=0}^{M-1} \frac{\partial \log p(y^m_{k(m)} | \mathbf{x}^m) }{\partial W_{ki}}.
%\label{eq:gradlogPycx}
%\end{equation}
%
In order to compute the full gradient $\nabla_\mathbf{W}\mathcal{F}(\mathcal{D};\Theta)$ we only need to compute each partial derivative represented in Eq.\ref{eq:gradlogPycx}. These partial derivatives can, in turn, be decomposed using the chain rule in Eq.~\ref{eq:chainRule}, as follows
%
\begin{align}
\frac{\partial \log p(y^m=c_{k(m)} | \mathbf{x}^m) }{\partial w_{ki}} & = \frac{\partial (\log \circ f_{k(m)} \circ \mathbf{g})(\mathbf{x}^m) }{\partial w_{ki}}\nonumber\\ & = \sum_{k'=1}^{K}\frac{\partial \log \circ f_{k(m)}(\mathbf{z}^m)}{\partial z^m_{k'}}\frac{\partial z^m_{k'}}{\partial w_{ki}}.
\label{eq:gradlogPycx2}
\end{align}
%\begin{align}
%\frac{\partial \log p(y^m_{k(m)} | \mathbf{x}^m) }{\partial W_{ki}} & = \frac{\partial (\log \circ f_{k(m)} \circ \mathbf{g})(\mathbf{x}^m) }{\partial W_{ki}}\nonumber\\ & = \sum_{k'=0}^{K-1}\frac{\partial \log \circ f_{k(m)}(\mathbf{z}^m)}{\partial z_{k'}}\frac{\partial z_{k'}}{\partial W_{ki}}.
%\end{align}
%
Now we only need to compute the two simpler derivatives that compose the latter expression in Eq.\ref{eq:gradlogPycx2}. The first of these derivatives %($\frac{\partial \log \circ f_{k(m)}(\mathbf{z}^m)}{\partial z^m_{k'}}$) 
is the derivative of the cost function with respect to $z^m_{k'}$. This term
could actually be further
decompose using the chain rule again, but it is easily shown to be given by
%
\begin{align}
\frac{\partial \log \circ f_{k(m)}(\mathbf{z}^m)}{\partial z^m_{k}} = 
  \begin{cases}
      1 - \tilde{z}_k^m  &  \mbox{ if } k = k(m)\\ 
      -\tilde{z}_k^m    &  \mbox{ otherwise }.
  \end{cases}
  \label{eq:patialSoftmax}
\end{align}
%
Since $z^m_{k'}$ only depends on the weight $w_{ki}$ in a linear way (see the graph in Fig.\ref{fig:LayerP}), the second derivative in Eq.\ref{eq:gradlogPycx2} is given by
\begin{align}
\frac{\partial z^m_{k'}}{\partial w_{ki}} = 
  &\begin{cases}
      x_i^m  &  \mbox{ if } k = k'\\ 
      0    &  \mbox{ otherwise },
  \end{cases}
  \label{eq:partialLinear}
\end{align}
which is the input signal at the weight $w_{ki}$ (see Fig.\ref{fig:LayerP}).
If we plug the latter two equations into Eq. \ref{eq:gradlogPycx2} to obtain each 
derivative for $w_{ki}$, we get the following matricial closed form for 
$\nabla_\mathbf{W}\mathcal{F}(\mathcal{D};\Theta)$
%the gradient of $\mathcal{F}(\mathcal{D};\Theta)$

\begin{equation}
\nabla_\mathbf{W}\mathcal{F}(\mathcal{D};\Theta) = -\frac{1}{M}\sum_{m=1}^{M} \Big(\mathrm{\mathbf{1}}_{k(m)} - \tilde{\mathbf{z}}^m \Big) \left(\mathbf{x}^m\right)^T,  
\label{gradWeigths}
\end{equation}
%
where $\mathrm{\mathbf{1}}_{k(m)} \in \mathbb{R}^{K}$ is a vector of zeros with a one in $k(m)$, which is 
the index of the correct class for the example $m$. In order to compute
the derivatives of the cost function with respect to the bias parameters $b_{k}$

\begin{align}
\frac{\partial \mathcal{F}(\mathcal{D};\Theta)}{\partial b_{k}} & = \sum_{k'=1}^{K}\frac{\partial \log \circ f_{k(m)}(\mathbf{z}^m)}{\partial z^m_{k'}}\frac{\partial z^m_{k'}}{\partial b_{k}},
\end{align}
%
we only need to compute one additional derivative
\begin{align}
\frac{\partial z_{k'}}{\partial b_{k}} = 
  &\begin{cases}
      1  &  \mbox{ if } k = k'\\ 
      0  &  \mbox{ otherwise },
  \end{cases} 
  \label{eqn:eqsilonq}
\end{align}
%
which is the input signal at the weight $b_k$ (see Fig.\ref{fig:LayerP}).
This leads us to the last gradient expression

\begin{equation}
\nabla_\mathbf{b}\mathcal{F}(\mathcal{D};\Theta) = -\frac{1}{M}\sum_{m=1}^{M} \Big(\mathrm{\mathbf{1}}_{k(m)} - \tilde{\mathbf{z}}^m \Big).
\label{eq:gradBias}
\end{equation}

Note that $\nabla_\mathbf{W}\mathcal{F}(\mathcal{D};\Theta)$ and $\nabla_\mathbf{b}\mathcal{F}(\mathcal{D};\Theta)$ can be decomposed into shared terms, 
$\mathrm{\mathbf{1}}_{k(m)} - \tilde{\mathbf{z}}^m$
%\begin{align}
%\mathrm{\mathbf{1}}_{k(m)} - \tilde{\mathbf{z}}^m,
%\end{align}
%
(consisting of the %back-propagation of the 
gradient of the cost function %until 
with respect to $\mathbf{z}^m$), 
%$\nabla_\mathbf{z}^m\mathcal{F}(\mathcal{D};\Theta)$
%that multiply 
and terms specific to the parameter under consideration (the forward input into that parameters).
The product of these two elements (the forward input and the back-propagation of the derivative of the cost function) is the main idea behind the Back-propagation algorithm. The Back-propagation algorithm uses the chain rule to back-propagate the gradient of the cost function %through networks
 and it is specially useful to compute gradients in complex neural networks, as we will see %exemplified in more detail 
in Section \ref{sec:deep_forward}.

\subsection{Changing the non-linearity}

Note that the elementary functions $\mathbf{g}(\mathbf{x})$ and $f_k(\mathbf{z})$ of our network (Fig.\ref{eq:gradlogPycx}) can be easily changed. 
In fact, if is common to use various types of nonlinear functions $f_k(\mathbf{z})$ in feed-froward networks. 

Before we get into deeper models it is worth to revise the case in which our
non-linear function $f_k(\mathbf{z})$ is the logistic function
\begin{equation}
\tilde{z}_k = f_k(z_k) = \frac{1}{1+\exp(-z_k)}.
\label{eq:sigmoid}
\end{equation}
%
In contrast with the soft-max function (in Eq.\ref{eq:softmax}), Eq.\ref{eq:sigmoid} is a element-wise  sigmoidal function, which is commonly used inside deeper networks. The derivative the logistic function is given by 
\begin{equation}
\frac{\partial \tilde{z}_k}{\partial z_k} = \tilde{z}_{k} (1-\tilde{z}_{k}).
\label{eq:partsigmoid}
\end{equation}
%
These expressions will be helpful in further sections, where we will use deeper and more complex neural models with sigmoidal units.

\begin{exercise}
Get in contact with the multi-layer perceptron (MLP) class in Numpy and see
that for a single layer this is simply a log-linear model. Revisit the
sentiment classification exercise of day one. Reformulate train and test data
in a way suitable for the exercises of today.  
\begin{python}
import numpy as np
import lxmls.readers.sentiment_reader as srs  
scr     = srs.SentimentCorpus("books")
train_x = scr.train_X.T
train_y = scr.train_y[:, 0]
test_x  = scr.test_X.T
test_y  = scr.test_y[:, 0]
\end{python}
%
Load the MLP and SGD code and create a single layer model by specifying the
number of inputs, outputs and the type of layer. Note that the number of inputs
equals the number of features and the number of outputs the number of classes
(2).
%
\begin{python}
# Define MLP (log linear)
import lxmls.deep_learning.mlp as dl
import lxmls.deep_learning.sgd as sgd
# Model parameters
geometry = [train_x.shape[0], 2]
actvfunc = ['softmax']
# Instantiate model
mlp      = dl.NumpyMLP(geometry, actvfunc)
\end{python}
Put a breakpoint inside of the lxmls/deep\_learning/mlp.py function and debug
step by step. Identify the introduced formulas and the computation of the
gradients (these are to be completed in the next exercise). 
\begin{python}
# Play with the untrained MLP forward
hat_train_y = mlp.forward(train_x) 
hat_test_y  = mlp.forward(test_x) 
# Compute accuracy
import lxmls.deep_learning.sgd as sgd
acc_train = sgd.class_acc(hat_train_y, train_y)[0]
acc_test  = sgd.class_acc(hat_test_y, test_y)[0]
print "Untrained Log-linear Accuracy train: %f test: %f"%(acc_train,acc_test)
\end{python}
\end{exercise}

\section{Going Deeper than Log-linear by using Composition}
\label{sec:deep_forward}

\subsection{Multilayer Perceptron}

We have seen that just using the chain rule we can easily compute gradients for
compositions of two functions (one non-linear and one linear). However, there
was nothing in the derivation that would stop us from composing more than two
functions. 
%\mla{ADD?: In fact, this flexibility is one of the advantage of the neural models.}
Let us imagine a general case in which we compose $n=1 \cdots N$
pairs of linear and non-linear functions 
%We will have \mla{Referir/introduzir o conceito de multilayer perceptin (MLP)??}
%
\begin{equation}
p(y=c_k|{x}) = (f_k^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^1 \circ \mathbf{g}^1)(\mathbf{x}),
\label{eq:FeedForward}
\end{equation}
%\begin{equation}
%p(y_k|\mathbf{x}) = (f_k^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^1 \circ \mathbf{g}^1)(\mathbf{x}),
%\label{eq:FeedForward}
%\end{equation}
%
\noindent where we denote each linear function as

\begin{equation}
\mathbf{z}^n = \mathbf{g}^n(\tilde{\mathbf{z}}^{n-1}) = \mathbf{W}^n \tilde{\mathbf{z}}^{n-1} + \mathbf{b}^n 
\end{equation}
%
\noindent and each non-linear function as 

\begin{equation}
\tilde{\mathbf{z}}^n = \mathbf{f}^n(\mathbf{z}^n).
\end{equation}
%
This feed-forward network is a called \textit{multilayer perceptron} (MLP), where each pair of non-linear and linear transformation, expressed as $(\mathbf{f}^n \circ \mathbf{g}^n)$, is a \textit{layer} with parameters $\Theta^n={\mathbf{W}^n, \mathbf{b}^n}$. We will use in all the internal (hidden) layers $\mathbf{f}^1 \cdots \mathbf{f}^{N-1}$ sigmoid functions as in Eq.\ref{eq:sigmoid} and in the final layer $\mathbf{f}^N$ we will use a softmax, so that the final output
can be interpreted as a distribution over $K$ classes (see the computation graph of Fig.\ref{fig:LayerP2}). The parameters of our
model are going to be the weights and bias of each layer
$\Theta=\{\mathbf{W}^1, \mathbf{b}^1, \cdots \mathbf{W}^N, \mathbf{b}^N\}$.

\begin{figure}
\centering
\includegraphics[scale=0.4]{figs/deep_learning/LayerP2.pdf}
\caption{Detail of propagation of error in weight $w_{ji}$ from layer $n$ to
layer $n+1$. Note that, while only $z_j^n$ is affected by the value in $w_{ji}$, all outputs
of layer $n+1$ are affected by the value in $w_{ji}$.
\mla{TODO: update figure.}}
\label{fig:LayerP2}
\end{figure}

Following the same steps as in the previous section, we will use a cost function be given by %for SGD will look like this
\begin{align}
\mathcal{F}(\mathcal{D};\Theta) & = -\frac{1}{M}\sum_{m=1}^{M} \log p(y^m=c_{k(m)} | \mathbf{x}^m)\nonumber\\ & = -\frac{1}{M}\sum_{m=1}^{M} (\log \circ f_{k(m)}^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^1 \circ \mathbf{g}^1)(\mathbf{x}^m).
\end{align}
%\begin{align}
%\mathcal{F}(\mathcal{D};\Theta) & = -\frac{1}{M}\sum_{m=0}^{M-1} \log p(y^m | \mathbf{x}^m)\nonumber\\ & = -\frac{1}{M}\sum_{m=0}^{M-1} (\log \circ f_{k(m)}^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^1 \circ \mathbf{g}^1)(\mathbf{x}^m)
%\end{align}

\subsection{Back-propagation: an overview}
To compute the gradient with respect the parameters of the $n$-th layer, we just need to apply the chain rule as in the previous section, consecutively. 
%selecting 
%%%the output of the parameter under consideration as
% an intermediate variable to decompose  
%and repeating the process until we obtain the final derivative. 
Fortunately, we
do not need to repeat this procedure for each layer as it is easy to spot a recursive
rule (using the Back-propagation method) that is valid for many neural models, including feed-forward networks (such as MLPs) as well as recurrent neural networks (RNNs) with minor modifications.  

The Back-propagation method, which is given in Algorithm \ref{algo:backprop} for a the case of an MLP, consist of the following steps:
\begin{itemize}
\item The \textit{forward pass} step, where the input signal is injected though the network  in a forward fashion (see steps 5-7 in Alg.~\ref{algo:backprop})
\item The \textit{back-propagation} step, where the derivative of the cost function (also called error) is injected back through the network and back-propagated cording the derivative rules (see steps 8-17 in Alg.~\ref{algo:backprop})
\item Finally, the gradient with respect to the parameters are computed by multiplying the input signal from the forward pass and the back-propagated error signal, at the corresponding places in the network (step 18 in Alg.~\ref{algo:backprop})
\item Given the gradients computed in the previous step, the model weights can then be easily update according a specified learning rule (step 19 in Alg.~\ref{algo:backprop} uses a mini-batch SGD udaptation rule).
\end{itemize}
%Alg.~\ref{algo:backprop} uses a mini-batch SGD updation rule. 
The main step of the method is the back-propagation step, where one has to compute the back-propagation recursion rules for a specific network.
Next section carefully deduce this recursion rules, for the present MLP model. 

%\mla{TODO: write a small overview of the back-propagation, using Alg. 14 (and Fig.5.2) as a reference}
%\noindent Let us consider the split at $\mathbf{z}^n$

%\begin{equation}
%\log p(y^m | \mathbf{x}^m) = (\log \circ f_{k(m)}^N \circ \mathbf{g}^N \circ \cdots \mathbf{f}^{n+1}) \circ (\mathbf{g}^{n+1} \circ \cdots \mathbf{f}^{1} \circ \mathbf{g}^{1})
%\end{equation}
%
%\begin{equation}
%\log p(y^m | \mathbf{x}^m) = (\log \circ f_{k(m)}^N \circ \mathbf{g}^N \circ \cdots \mathbf{f}^{n}) \circ (\mathbf{g}^{n} \circ \cdots \mathbf{f}^{1} \circ \mathbf{g}^{1})
%\end{equation}

\subsection{The Back-propagation Recursion}
Let us start by computing the derivative for an arbitrary $n$-th hidden layer. Similarly to what was done for the single-layer network in the previous section, we will
split the chain rule at the output of the $n$-th linear transformation ($\mathbf{z}^n$), in order to compute the derivatives with respect to the parameters of that linear transformation ($\mathbf{W}^n, \mathbf{b}^n$). Since anything before the $n$-th layer does not depend on $w_{ji}^n$, the chain rule split at $\mathbf{z}^n$ will be given by

\begin{align}
\frac{\partial \log p(y^m | \mathbf{x}^m)}{\partial w_{ji}^n} & = \sum_{j'=1}^J \frac{\partial}{\partial z^{m,n}_{j'}} (\log \circ f_{k(m)}^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^{n+1} \circ \mathbf{g}^{n+1} \circ \mathbf{f}^{n})(\mathbf{z}^{m,n})\frac{\partial z^{m,n}_{j'}}{\partial w_{ji}^n}\nonumber\\ & = \sum_{j'=1}^J e^{m,n}_{j'} \frac{\partial z^{m,n}_{j'}}{\partial w_{ji}^n}\nonumber\\ & = e^{m,n}_{j}\frac{\partial z^{m,n}_{j}}{\partial w_{ji}^n},
\label{eq:partialfn}
\end{align}
%
where the last equality uses the fact that $z^{m,n}_{j'}$ only depends on
$w_{j'i}^n$ if $j=j'$ and thus the sum over $j'$ disappears. The second term in Eq.\ref{eq:partialfn}, ${\partial z^{m,n}_{j}}/{\partial w_{ji}^n}=\tilde(z)_{j}^{m,(n-1)}$, is the forward input at $w_{j'i}^n$, while the term $e^{m,n}_j$
is the derivative of the cost with respect to ${z}^{m,n}_j$ (the intermediate output of that linear transformation, for the $m$-th example),  given by

\begin{equation}
e^{m,n}_j =  \frac{\partial \mathcal{F}(\mathcal{D};\Theta)}{\partial z^{m,n}_{j}} = \frac{\partial}{\partial z^{m,n}_{j}} (\log \circ f_{k(m)}^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^{n+1} \circ \mathbf{g}^{n+1} \circ \mathbf{f}^{n})(\mathbf{z}^{m,n}).
\label{eq:e_mn_j}
\end{equation}
%
The error of Eq.\ref{eq:e_mn_j} can be computed recursively by back-propagating the derivative of the cost function through the network.
To get the expression for this back-propagation recursion, we will consider the same derivative one layer upwards in the
computation graph  

\begin{equation}
e^{m,n+1}_k = \frac{\partial}{\partial z^{m,n+1}_{k}} (\log \circ f_{k(m)}^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^{n+1})(\mathbf{z}^{m,n+1}).
\end{equation}
%
%\begin{figure}
%\centering
%\includegraphics[scale=0.4]{figs/deep_learning/LayerP2.pdf}
%\caption{Detail of propagation of error in weight $w_{ji}$ from layer $n$ to
%layer $n+1$. Note that, while only $z_j^n$ is affected by the value in $w_{ji}$, all outputs
%of layer $n+1$ are affected by the value in $w_{ji}$.
%\mla{TODO: update figure.}}
%\label{fig:LayerP2}
%\end{figure}
%
\noindent If we apply the chain rule twice, we obtain the
recursion rule that defines how the derivatives of the cost function are back-propagated through the network\footnotemark\footnotetext{See Fig.\ref{fig:LayerP2} and the annex at the end of this chapter
for a detailed derivation}

\begin{equation}
e^{m,n}_j = \sum_{k=1}^K e^{m,n+1}_k \frac{\partial z^{m,n+1}_k}{\partial \tilde{z}_{j}^{m,n}}\frac{\partial \tilde{z}^{m,n}_{j}}{\partial z_{j}^{m,n}},
\label{eq:chainRulRecursion}
\end{equation}
%
%\noindent It only rests to compute the derivative
where %the first term is given by
\begin{equation}
\frac{\partial z^{m,n+1}_k}{\partial \tilde{z}_{j}^{m,n}} = w_{kj}^{n+1} 
\end{equation}

\noindent and ${\partial \tilde{z}^{m,n}_{j}}/{\partial z_{j}^{m,n}}$ is the derivative of the sigmoid, given in Eq.\ref{eq:partsigmoid}. This leads to the following back-propagation recursion for the derivative
%with respect to $w_{ji}$

\begin{equation}
e^{m,n}_j = \sum_{k=1}^K e^{m,n+1}_k w_{kj}^{n+1}\tilde{z}^{m,n}_{j}(1-\tilde{z}^{m,n}_{j}),
\end{equation}
%
\noindent where $e^{m,N}_j$ (given by Eq.~\ref{eq:patialSoftmax}) is the derivative that is going to be injected in the network in a backward fashion. 
%\mla{TODO: 1) finalizar a derivada com o produto do erro com o input do forward pass, 2)sumarizar equacoes (e Backpropagation) em formato matricial}
We can express these equations in matrix form 
%we get the 
and get the Back-propagation rules that are integrate in the mini-batch SGD method of Alg.\ref{algo:backprop}. 

\begin{algorithm}[th!]
\label{algo:backprop}

   \caption{Mini-batch SGD with Back-Propagation \label{alg:maxent_gd}}

\begin{algorithmic}[1]

   \STATE {\bfseries input:} 
   %Data $\mathcal{D}$, Feed-forward of $N$ layers, with parameters $\Theta=\{\mathbf{W}^1, \mathbf{b}^1, \cdots \mathbf{W}^N, \mathbf{b}^N\}$, number of rounds $T$, $B$ mini-batches of size $M$, learning rate $\eta$
Data $\mathcal{D}=\{\mathcal{D}_1,\mathcal{D}_2,...,\mathcal{D}_B\}$ split into $B$ mini-batches of size $M'$%(in each data mini-batch $\mathcal{D}_b$ the example indicies $m$ range from 1 to $M'$)
, MLP of $N$ layers, with parameters $\Theta=\{\mathbf{W}^1, \mathbf{b}^1, \cdots \mathbf{W}^N, \mathbf{b}^N\}$, number of rounds $T$, learning rate $\eta$

   \STATE initialize parameters $\Theta$ randomly 

	\FOR{$t=1$ {\bfseries to} $T$}
	\FOR{$b=1$ {\bfseries to} $B$}

	\vspace{0.3cm}
	\FOR{$m=1$ {\bfseries to} $M'$}
	\STATE Compute the {forward pass} for each of the $M'$ examples in batch $b$:
    $$p(y^m|\mathbf{x}^m) \equiv \tilde{\mathbf{z}}^{m,N} = (f_k(m)^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^1 \circ \mathbf{g}^1)(\mathbf{x}^m),$$
	 and keep all intermediate non-linear outputs $\tilde{\mathbf{z}}^{m,1} \cdots \tilde{\mathbf{z}}^{m,N}$.
	\ENDFOR	
	\vspace{0.3cm}
    %\STATE \textbf{Back-propagate} $\nabla_{\mathbf{z}^{m,n}}\mathcal{F}(\mathcal{D}_b;\Theta)$ 
	\FOR{$n=N$ {\bfseries to} $1$}
        \IF{n==N} 
		\FOR{$m=1$ {\bfseries to} $M'$}
        \STATE {Initialize the error at last layer, for each example $m$:
        $$\mathbf{e}^{m,N} = \Big(\mathrm{\mathbf{1}}_{k(m)} - \tilde{\mathbf{z}}^{m,N} \Big).$$}
		\ENDFOR	
        \ELSE
   		\FOR{$m=1$ {\bfseries to} $M'$}
		\STATE {Back-propagate} the error one layer, for each example $m$:  
        $$\mathbf{e}^{m,n} = \Big((\mathbf{W}^{n+1})^T \mathbf{e}^{m,n+1}\Big) \odot \tilde{\mathbf{z}}^{m,n} \odot (\mathbf{\mathrm{1}}-\tilde{\mathbf{z}}^{m,n}),$$
        where $\odot$ is the element-wise product and the $\mathbf{\mathrm{1}}$ is replicated to match the size of $\tilde{\mathbf{z}}^n$.
		\ENDFOR	
        \ENDIF 

		\vspace{0.3cm}
        \STATE {Compute the gradients} using the back-propagated errors and the inputs from the forward pass

        $$\nabla_{\mathbf{W}^n}\mathcal{F}(\mathcal{D_b};\Theta)  = -\frac{1}{M'} \sum_{m=1}^{M'} \mathbf{e}^{m,n} \cdot \left(\tilde{\mathbf{z}}^{m,n-1}\right)^T,$$ 
        $$\nabla_{\mathbf{b}^n}\mathcal{F}(\mathcal{D_b};\Theta)  = - \frac{1}{M'} \sum_{m=1}^{M'} \mathbf{e}^{m,n}.$$  

		\vspace{0.3cm}
        \STATE Update the parameters 
            $$\mathbf{W}^n \leftarrow \mathbf{W}^n - \eta \nabla_\mathbf{W^n}\mathcal{F},$$ 
            $$\mathbf{b}^n \leftarrow \mathbf{b}^n - \eta \nabla_\mathbf{b^n}\mathcal{F}.$$ 

	\ENDFOR

	\ENDFOR
	\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{exercise}
Go to lxmls/deep\_learning/mlp.py:class NumpyMLP:def grads() and complete the
code of the NumpyMLP class with the Backpropagation recursion that we just saw.
Once you are done. Try different network geometries by increasing the number of
layers and layer sizes e.g.
\begin{python}
# Model parameters
geometry = [train_x.shape[0], 20, 2]
actvfunc = ['sigmoid', 'softmax'] 
# Instantiate model
mlp      = dl.NumpyMLP(geometry, actvfunc) 
\end{python}
You can test the different models with the same sentiment analysis problem as
in Exercise 6.1. 
\begin{python}
# Model parameters
n_iter = 5
bsize  = 5
lrate  = 0.01
# Train
sgd.SGD_train(mlp, n_iter, bsize=bsize, lrate=lrate, train_set=(train_x, train_y))
acc_train = sgd.class_acc(mlp.forward(train_x), train_y)[0]
acc_test  = sgd.class_acc(mlp.forward(test_x), test_y)[0]
print "MLP (%s) Amazon Sentiment Accuracy train: %f test: %f" % (geometry, acc_train,acc_test)
\end{python}
\end{exercise}

\subsection{Some final reflections on Backpropagation}

If you are new to the neural network topic, this is about the most important
piece of theory you should learn about deep learning. Here are some reflections
that you should keep in mind.

\mla{TODO: add ...}
\begin{itemize}
\item Thanks to the multi-layer structure and the chain rule, Backpropagation allows models that compose linear and non-linear functions with any depth (in principle\footnotemark). 

\item The formulas are also valid for other cost functions and output layer non-linearities with minor modifications. It is only necessary to compute the equivalente of Eq.~\ref{eq:patialSoftmax}. 

\item The formulas are also valid for hidden non-linearities other than the sigmoid. Element-wise non-linear transformations still allow the simplification in Eq: \ref{eq:chainRulRecursion}. With little effort it is also possible to deal with other cases.

\item However, there is an important limitation: Unlike the log-linear models, the optimization problem is \textit{non convex}. This removes some formal guarantees, most importantly we can get trapped in local minima during training.
\end{itemize}

\footnotetext{Not exactly, since it is possible to run into numerical problems.}

% THIS WILL BE COMMENTED UNTIL THE FINAL UPDATE
%
%\subsection{A Note on Pre-Trainining}
%
%If you already have some experience with neural networks you might have
%realised that all what we show here is classic MLP theory, which is 30-40 years
%old. Indeed, it could be argued that, at the core, many modern deep learning
%applications are just classical neural networks theory with more data and more
%computing power. One of the novelties that came along the deep learning wave of
%research is pre-training with the Restricted Boltzmann Machine (RBM) paradigm.
%The cost function for a MLP of many layers becomes too complex and, as
%mentioned before, local minima and non-convexity make training of deep MLPs
%problematic. 
%
%The RBM paradigm allows to pre-train a MLP in \textit{unsupervised} fashion
%i.e. with out reference labels $\mathbf{y}^m$, which has been shown to improve
%posterior training. However, current state of the art systems do not always use
%RBM pre-training, resort to simpler types of smart-initializations or use none.
%Fort his reason, we will only see pre-training as an advance topic.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Deriving gradients and GPU code with Theano}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{An Introduction to Theano}

As you may have observed, the speed of SGD training for MLPs slows down
considerably when we increase the number of layers. One reason for this is that
the code that we use here is not very optimized. It is thought for you to
learn the basic principles. Even if the code was more optimized, it would still be
very slow for reasonable network sizes. The cost of computing each
linear layer is proportional to the dimensionality of the previous and current
layers, which in most cases will be rather large. 

For this reason most deep learning applications use Graphics Processing Units
(GPU) in their computations. This specialized hardware is normally used to
accelerate computer graphics, but can also be used for general computation
intensive tasks. However, we need to deal with specific interfaces and
operations in order to use a GPU. This is where Theano comes in. Theano is a
multidimensional symbolic expression python module with focus on neural
networks. It will provide us with the following nice features:

\begin{itemize}
\item Symbolic expressions: Express the operations of the MLP (forward pass, cost) symbolically, as mathematical operations rather than explicit code 
\item Symbolic Differentiation: As a consequence of the previous feature, we can compute gradients of arbitrary mathematical functions automatically.   
\item GPU integration: The code will be ready to work on a GPU, provided that you have one and it is active within Theano. It will also be faster on normal CPUs since the symbolic operations are compiled to C code. 
\item Theano is focused on Deep Learning, with an active community and several tutorials easily available.  
\end{itemize}

The only negative aspect is that we will have to learn to deal with Theano and
in particular working with symbolic representations. We will start right away
with some exercises.

\begin{exercise}
Get in contact with Theano. Learn the difference between a symbolic
representation and a function. Start by implementing the first layer of our
previous MLP in Numpy 
\begin{python}
# Numpy code
x        = test_x             # Test set 
W1, b1   = mlp.params[:2]     # Weights and bias of fist layer 
z1       = np.dot(W1, x) + b1 # Linear transformation
tilde_z1 = 1/(1+np.exp(-z1))  # Non-linear transformation  
\end{python}
Now we will implement this in Theano.  We start by creating the variables over
which we will produce the operations. For example the symbolic input is defined
as
\begin{python}
# Theano code. 
# NOTE: We use undescore to denote symbolic equivalents to Numpy variables. 
# This is no Python convention!.
import theano
import theano.tensor as T
_x = T.matrix('x')
\end{python}
Note that this variable does not have any particular value, nor a space
reserved in memory for it. It contains just a symbolic definition of what the
variable can contain. The particular values will be given when we use it to
compile a function. 

We could actually use the same definition format to define the weights and give
their particular values as inputs to the compiled function. However, since we
will be using a more complicated format in later exercises, we will use it here
as well. The \textit{shared} class allows to define variables that are shared
across functions. They are also given a concrete value so that we do not need
to give it for each function call. This format is therefore ideal for the
weights of our network.
\begin{python}
_W1 = theano.shared(value=W1, name='W1', borrow=True) 
_b1 = theano.shared(value=b1, name='b1', borrow=True, broadcastable=(False, True)) 
\end{python}
Now lets describe the operations we want to do with the variables. Again only
symbolically. This is done by replacing our usual operations by Theano symbolic
ones when necessary e. g. the internal product dot() or the sigmoid. Some
operations like e.g. $+$ are automatically recognized by Theano (operator
overloading). 
\begin{python}
_z1            = T.dot(_W1, _x) + _b1
_tilde_z1      = T.nnet.sigmoid(_z1)
# Keep in mind that naming variables is useful when debugging
_z1.name       = 'z1'
_tilde_z1.name = 'tilde_z1'
\end{python}
When debugging the code it is often useful to print the graph of computations.
\begin{python}
# Perceptron computation graph
theano.printing.debugprint(_tilde_z1)

sigmoid [@A] 'tilde_z1'
 |Elemwise{add,no_inplace} [@B] 'z1'
   |dot [@C] ''
   | |W1 [@D]
   | |x [@E]
   |b1 [@F]

\end{python}
It is important to keep in mind that, until this point, we do not have a
function we can use to produce any practical input. In order to obtain this we
have to compile this function by calling    
\begin{python}
layer1 = theano.function([_x], _tilde_z1)
\end{python}
Note the use of $[$ $]$ for the input variables, even if we just specify one
variable. We can now do a test to compare the Numpy and Theano implementations
and see that they give the same outputs.
\begin{python}
# Check Numpy and Theano match
if np.allclose(tilde_z1, layer1(x.astype(theano.config.floatX))):
    print "\nNumpy and Theano Perceptrons are equivalent"
else:
    raise ValueError, "Numpy and Theano Perceptrons are different"
\end{python}
\end{exercise}

\subsection{Symbolic Forward Pass}
In the previous section you have seen how to create symbolic Theano functions
with shared parameters. You have thus all you need to implement the whole
forward pass of a generic MLP in Theano.
\begin{exercise}
Complete the method \_forward() inside of the lxmls/deep\_learning/mlp.py:class
TheanoMLP. Note that this is called only once at the initialization of the
class. To debug your implementation put a breakpoint at the \_\_init\_\_
function call. Hint: Note that this is very similar to NumpyMLP.forward().
You just need to keep track of the symbolic variable representing the output of
the network after each layer is applied and compile the function at the end.
After you are finished instantiate a Theano class and check that Numpy and
Theano forward pass are the same. 

\begin{python}
mlp_a = dl.NumpyMLP(geometry, actvfunc)
mlp_b = dl.TheanoMLP(geometry, actvfunc)
\end{python}

Bear in mind that you can use previous experience to debug this.

\end{exercise}

\subsection{Symbolic Differentiation}
In the previous section we compiled the forward pass of a MLP. In this section
we will do the same with the cost used for training. We will also derive the
gradients although this will be trivial once we have the cost function compiled.     
\begin{exercise}
We first see an example that does not use any of the code in TheanoMLP but
rather continues from what you wrote in exercise 6.3. In this exercise you
completed a sigmoid layer with Theano. To get some values for the weights we
used the first layer of the network you trained in 6.2. now we are going to use
the second layer as well. This is thus assuming that your network in 6.2 has
only two layers e.g. the recommended geometry (I, 20, 2). Make sure this is the
case before starting this exercise.  

For the sake of clarity, lets write here the part of Ex. 6.2 that we had completed
\begin{python}
# Get the values from our MLP from Ex 6.2
W1, b1   = mlp.params[:2]     # Weights and bias of fist layer 
# First layer symbolic variables
_x  = T.matrix('x')
_W1 = theano.shared(value=W1, name='W1', borrow=True) 
_b1 = theano.shared(value=b1, name='b1', borrow=True, broadcastable=(False, True)) 
# First layer symbolic expressions
_z1       = T.dot(_W1, _x) + _b1
_tilde_z1 = T.nnet.sigmoid(_z1)
\end{python}
Now we just need to complete this with the second layer, using a softmax non-linearity
\begin{python}
W2, b2  = mlp.params[2:]     # Weights and bias of second (and last!) layer 
# Second layer symbolic variables
_W2 = theano.shared(value=W2, name='W2', borrow=True) 
_b2 = theano.shared(value=b2, name='b2', borrow=True, broadcastable=(False, True)) 
# Second layer symbolic expressions
_z2       = T.dot(_W2, _tilde_z1) + _b2
_tilde_z2 = T.nnet.softmax(_z2.T).T
\end{python}
With this, we could compile a function to obtain the output of the network
symb\_tilde\_z2 for a given input symb\_x. In this exercise we are however
interested in obtaining the misclassification cost. This is given in Eq:
\ref{eq:CostLogPos}. First we are going to need the symbolic variable for the
correct output
\begin{python}
_y = T.ivector('y')
\end{python}
The minus posterior probability of the class given the input is the same as
selecting the $k(m)$-th softmax output, were $k(m)$ is the index of the correct
class for $x^m$. If we want to do this for a vector $\mathbf{y}$ containing $M$
different examples, we can write this as
\begin{python}
_F = -T.mean(T.log(_tilde_z2[_y, T.arange(_y.shape[0])]))
\end{python}
Now obtaining a function that computes the gradient could not be easier.
\begin{python}
_nabla_F = T.grad(_F, _W1) 
nabla_F  = theano.function([_x, _y], _nabla_F) 
\end{python}
To finish this exercise have a look at the TheanoMLP class. As you may realise it just implements what is shown above for the generic case of $N$ layers
\end{exercise}

\subsection{Symbolic mini-batch update}

The code above is used in the normal SGD\_train when utilizing Theano. Even if
you do not have a GPU configured, it should be run faster than our Numpy
version, particularly for large batch sizes. There is however a
way to make it run even faster by implementing not only gradient computation
but the whole batch update of SGD inside Theano. For this we need also to share
the whole training set, or a very large mega-batch of it. 

\begin{exercise}
Let us first have an understanding of handling train/test data inside the Theano
computation graph. One important aspect to take into account is that both type
and shape of the data have to match their corresponding graph variables. This is
the main source of errors when you are starting with Theano. 
\begin{python}
# Cast data into the types and shapes used in the Theano graph
train_x = train_x.astype(theano.config.floatX)
train_y = train_y.astype('int32')
\end{python}
Note the Theano type theano.config.floatX. This will automatically switch
between float32 (GPU) and float64 (CPU).

To use data in a Theano computation graph, we use the theano.shared variable.
This will also push data into the GPU, if used.
\begin{python}
_train_x = theano.shared(train_x, 'train_x', borrow=True)
_train_y = theano.shared(train_y, 'train_y', borrow=True)
\end{python}
Once this is done, we can create and compile functions using these variables.
One simple but useful function is a function returning a mini-batch of data 
of size bsize given the mini-batch index.  
\begin{python}
_i             = T.lscalar()
get_tr_batch_y = theano.function([_i], _train_y[_i*bsize:(_i+1)*bsize]) 
\end{python}
Compile this function and observe its behaviour. It will be necessary for the next exercise.
\end{exercise}

\begin{exercise}
The mini-batch function in the previous exercise is the key to fast batch
update. This is combined with the \emph{updates} argument of theano.function. The input to this argument,
is a list of tuples with each parameter and update rule. This can be compactly
defined using list comprehensions.
\begin{python}
mlp_c   = dl.TheanoMLP(geometry, actvfunc)
_x      = T.matrix('x')
_y      = T.ivector('y')
_F      = mlp_c._cost(_x, _y)
updates = [(par, par - lrate*T.grad(_F, par)) for par in mlp_c.params]
\end{python}

This can be now combined with the givens argument of theano.function. This maps
input and target to other variables. In this case a mini-batch of inputs and
targets given an index. 
\begin{python}
_j      = T.lscalar()
givens  = { _x : _train_x[:, _j*bsize:(_j+1)*bsize], 
            _y : _train_y[_j*bsize:(_j+1)*bsize] }
\end{python}

With updates and givens, we can now define the batch update function. This will
return the cost of each batch and update the MLP parameters at the same time
using updates
\begin{python}
batch_up = theano.function([_j], _F, updates=updates, givens=givens)
n_batch  = train_x.shape[1]/bsize  + 1
\end{python}
Once we have defined this, we can compare speed and accuracy of the Numpy
and simple gradient versions using

\begin{python}
import time
# Model
geometry = [train_x.shape[0], 20, 2]
actvfunc = ['sigmoid', 'softmax'] 

# Numpy MLP
mlp_a     = dl.NumpyMLP(geometry, actvfunc)
init_t    = time.clock()
sgd.SGD_train(mlp_a, n_iter, bsize=bsize, lrate=lrate, train_set=(train_x, train_y))
print "\nNumpy version took %2.2f sec" % (time.clock() - init_t)
acc_train = sgd.class_acc(mlp_a.forward(train_x), train_y)[0]
acc_test  = sgd.class_acc(mlp_a.forward(test_x), test_y)[0]
print "Amazon Sentiment Accuracy train: %f test: %f\n" % (acc_train, acc_test)

# Theano grads 
mlp_b  = dl.TheanoMLP(geometry, actvfunc)
init_t = time.clock()
sgd.SGD_train(mlp_b, n_iter, bsize=bsize, lrate=lrate, train_set=(train_x, train_y))
print "\nCompiled gradient version took %2.2f sec" % (time.clock() - init_t)
acc_train = sgd.class_acc(mlp_b.forward(train_x), train_y)[0]
acc_test  = sgd.class_acc(mlp_b.forward(test_x), test_y)[0]
print "Amazon Sentiment Accuracy train: %f test: %f\n" % (acc_train, acc_test)

# Theano batch update
init_t = time.clock()
sgd.SGD_train(mlp_c, n_iter, batch_up=batch_up, n_batch=n_batch)
print "\nTheano compiled batch update version took %2.2f" % (time.clock() - init_t)
acc_train = sgd.class_acc(mlp_c.forward(train_x), train_y)[0]
acc_test  = sgd.class_acc(mlp_c.forward(test_x), test_y)[0]
print "Amazon Sentiment Accuracy train: %f test: %f\n"%(acc_train,acc_test)
\end{python}
As you may observe, just computing the gradients with Theano may not lead to
a decrease, but rather an increase in computing time. To maximally exploit
the power of Theano, it is necessary to bundle both computations and data 
together using approaches like the compiled batch update.
\end{exercise}

\section*{Annex: Backpropagation in Detail}

This section demonstrates that, for the feed-forward
network defined in Eq.~\ref{eq:FeedForward}, if we define the derivatives of
the error with respect to to adjacent linear layer outputs
$\mathbf{z}^{m,(n+1)}$ and $\mathbf{z}^{m,n}$, given by

\begin{equation}
e^{m,n}_j = \frac{\partial}{\partial z^{n}_{j}} (\log \circ f_{k(m)}^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^{n+1} \circ \mathbf{g}^{n+1} \circ \mathbf{f}^{n})(\mathbf{z}^{m,n}) 
\label{eq:endetail}
\end{equation}

\noindent and 

\begin{equation}
e^{m,n+1}_k = \frac{\partial}{\partial z^{m,n+1}_{k}} (\log \circ f_{k(m)}^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^{n+1})(\mathbf{z}^{m,n+1})
\label{eq:enp1detail}
\end{equation}
%
\noindent then,
% for a feed-forward network with softmax output non-linearity and one-to-one intermediate non-linearities,
 the following relation holds

\begin{equation}
e^{m,n}_j = \sum_{k=1}^K e^{m,n+1}_k \frac{\partial z^{m,n+1}_k}{\partial \tilde{z}_{j}^{m,n}}\frac{\partial \tilde{z}^{m,n}_{j}}{\partial z_{j}^{m,n}}.
\label{eq:DetailchainRulRecursion}
\end{equation}

\noindent We departure from

\begin{equation}
e^{m,n}_j = \frac{\partial}{\partial z^{m,n}_{j}} (\log \circ f_{k(m)}^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^{n+1} \circ \mathbf{g}^{n+1} \circ \mathbf{f}^{n})(\mathbf{z}^{m,n}) 
\end{equation}

\noindent and we apply the chain rule at the non-linearity $\tilde{\mathbf{z}}^{m,n}$ to get

\begin{equation}
e^{m,n}_j =  \sum_{j'=0}^{J-1}\frac{\partial}{\partial \tilde{z}^{m,n}_{j'}} (\log \circ f_{k(m)}^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^{n+1} \circ \mathbf{g}^{n+1})(\tilde{\mathbf{z}}^{m,n})\frac{\partial \tilde{z}^{m,n}_{j'}}{\partial z_{j}^{m,n}}.
\end{equation}

Since this entails various one-to-one non-linear transformations, all
derivatives will be zero when $j\neq j'$, thus yielding 

\begin{equation}
e^{m,n}_j =  \frac{\partial}{\partial \tilde{z}^{m,n}_{j}} (\log \circ f_{k(m)}^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^{n+1} \circ \mathbf{g}^{n+1})(\tilde{\mathbf{z}}^{m,n})\frac{\partial \tilde{z}^{m,n}_{j}}{\partial z_{j}^{m,n}}.
\end{equation}

Let us now apply the chain rule at the linear output $\mathbf{z}^{m(n+1)}$. In this case the sum in the chain rule does not go away. It is easy to
understand why by looking at Figure \ref{fig:LayerP2}, that the weight $w_{kj}$
contributes not only to the variable $\tilde{z}^{m,n}_j$ but to all variables of the
next linear layer $z^{m,(n+1)}_k$, and thus to the possible error

\begin{equation}
e^{m,n}_j = \sum_{k=1}^K \frac{\partial}{\partial z^{m,n+1}_{k}} (\log \circ f_{k(m)}^N \circ \mathbf{g}^N \circ \mathbf{f}^{N-1} \circ \mathbf{g}^{N-1} \circ \cdots \mathbf{f}^{n+1})(\mathbf{z}^{m,(n+1)})\frac{\partial z^{m,n+1}_k}{\partial \tilde{z}_{j}^{m,n}}\frac{\partial \tilde{z}^{m,n}_{j}}{\partial z_{j}^{m,n}}.
\label{eq:partialfn4}
\end{equation}

\noindent By looking at Eqs.~\ref{eq:enp1detail} and \ref{eq:enp1detail} we can easily
spot the recursion in Eq.~\ref{eq:DetailchainRulRecursion}.  


