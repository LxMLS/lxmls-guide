

In this class, we relax the assumption that
the data points are independently and identically distributed (i.i.d.) 
by moving to a scenario of \emph{structured prediction}, where the inputs are assumed to have
temporal or spacial dependencies. We start by 
considering sequential models, which correspond to a \emph{chain structure}: for instance,
the words in a sentence. In this lecture, we will use part-of-speech
tagging as our example task.  

We start by defining the notation for this lecture in Section~\ref{notation}.
Afterwards, in section~\ref{hmm}, we focus on the well known Hidden Markov Models and in Section~\ref{ml} we describe how to estimate its parameters from labeled data. In Section \ref{decoding} we explain the inference
algorithms (Viterbi and Forward-Backward) for sequence models. These
inference algorithms will be fundamental for the rest of this lecture,
as well as for the next lecture on \emph{discriminative} training of sequence
models. In Section \ref{pos-tagging} we describe the task of 
Part-of-Speech tagging, and how the Hidden Markov Models are suitable for this task. 
Finally, in Section \ref{unsupervised} we 
address unsupervised learning of Hidden Markov Models through the Expectation Maximization
algorithm.

\section*{Today's assignment}

The assignment of today's class is to implement one inference algorithm for Hidden Markov Models, used to find the most likely hidden state sequence given an observation sequence.

\section{Notation}\label{notation}

In what follows, 
we assume a finite set of \emph{observation labels}, 
$\vocab := \{w_1,\ldots,w_J\}$,
and a finite set of \emph{state labels}, 
$\statevocab := \{c_1,\ldots, c_K\}$. We denote by $\vocab^*$, $\statevocab^*$ the two infinite sets of sequences obtained by grouping the elements of each label set including repetitions and the empty string $\varepsilon$\footnotemark\footnotetext{More formally, we say $\vocab^* := \{\varepsilon\} \cup \vocab \cup \vocab^2 \cup \ldots$ and $\statevocab^* := \{\varepsilon\} \cup \statevocab \cup \statevocab^2  \cup \ldots$ is the Kleene closure of each of the two sets above.}. 
Elements of $\vocab^*$ and $\statevocab^*$ 
are \emph{strings of observations} and \emph{strings of states}, 
respectively. 
Throughout this class, we 
assume our input set is $\X = \vocab^*$, 
and our output set is $\Y = \statevocab^*$. 
In other words, 
our inputs are observation sequences, 
$x = x_1 x_2 \ldots x_N$, 
for some $N \in \mathbb{N}$, where each $x_i \in \vocab$;
given such an $x$, we seek 
the corresponding state sequence, 
$y = y_1 y_2 \ldots y_N$, 
where each $y_i \in \statevocab$. We also consider two special states: the ${\tt start}$ symbol,
which starts the sequence, and 
the ${\tt stop}$ symbol, which ends the sequence. 

Moreover, in this lecture we will assume two scenarios:
\begin{enumerate}
\item \emph{Supervised learning.} We will 
train models from a sample set of paired observation and state sequences, $\mathcal{D}_L := \{(x^1,y^1), \ldots, (x^M,y^M)\} \subseteq \X \times \Y$.
\item \emph{Unsupervised learning.} We will
train models from the set of observations only, $\mathcal{D}_U := \{x^1, \ldots, x^M\} \subseteq \X$.
\end{enumerate}
Our notation is summarized in Table~\ref{tab:hmm_notation}.

%Let $\X = \{\sent^1, \ldots, \sent^D\}$ be a training set of independent
%and identically-distributed random variables. In this work $\sent^d$
%(for notation simplicity we will drop the superscript $d$ when
%considering an isolated example) corresponds to a sentence in natural
%language and decomposes as a sequence of observations of length $N$: $\sent = \obs_1 \ldots
%\obs_N$. Each $\obs_n$ is a discrete
%random variable (a \emph{word}),  taking a value $\vv$ from a
%finite vocabulary $\vocab$. Each $\sent$ has an unknown hidden
%structure $\hseq$  that we want to predict. The
%structures are sequences $\hseq = \hs_1 \ldots \hs_N$ of the same
%length $N$ as the observations. Each hidden state $\hs_n$ is a discrete
%random variable and can take a value $\hv$ from a discrete vocabulary $\hvocab$. 
%
%\afm{need to improve notation}

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|l|}
\hline
\multicolumn{2}{|c|}{Notation}\\
\hline
\hline
$\mathcal{D}_L$ & training set (including labeled data)\\
\hline
$\mathcal{D}_U$ & training set (unlabeled data only)\\
\hline
$M$  & number of training examples \\
\hline
$x = x_1 \ldots x_N$  & observation sequence \\
\hline
$y = y_1 \ldots y_N$  & state sequence \\
\hline

$N$  & length of the sequence \\
\hline
$x_i$ &  observation at position $i$ in the sequence, $i \in \{1,\ldots,N\}$\\
\hline
$y_i$ &  state at position $i$ in the sequence, $i \in \{1,\ldots,N\}$\\
%\hline
%${\tt start}, {\tt stop}$ & start and stop symbols\\ conceitos introduzidos na seccao seguinte
\hline
$\vocab$ & observation set\\
\hline 
$J$ & number of distinct observation labels\\
\hline 
$w_j$ & particular observation, $j \in \{1,\ldots,J\}$\\
\hline 
$\statevocab$ & state set\\
\hline 
$K$ & number of distinct state labels\\
\hline 
$c_k$ & particular state, $k \in \{1,\ldots,K\}$\\
\hline  
\end{tabular}
\end{center}
\label{tab:hmm_notation}
\caption{General notation used in this class}
\end{table}


\section{\label{hmm} Hidden Markov Models}
\input{pages/sequences/hmm.tex}

\section{\label{ml} Finding the Maximum Likelihood Parameters}
\input{pages/sequences/ml.tex}

\section{\label{decoding} Decoding a Sequence}
\input{pages/sequences/decoding.tex}

\section{\label{pos-tagging} Part-of-Speech Tagging (POS)}
\input{pages/sequences/pos.tex}

\section{\label{unsupervised} Unsupervised Learning of HMMs}
\input{pages/sequences/unsupervised.tex}


%\section{\label{hmm_special_state} HMM Initial and Final State}
%\input{pages/sequences/init_final_hmm.tex}

%\section{\label{hmm} Second order HMM}
%\input{pages/sequences/second_order_hmm.tex}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "../../guide.tex"
%%% End: 
